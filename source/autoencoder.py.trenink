from scipy.io import wavfile as wav
import matplotlib.pyplot as plt
from torch.autograd import Variable
import torchvision.transforms as transforms
import IPython.display as ipd
import numpy as np
import torch
import torch.utils.data as data_utils
from torch._six import int_classes as _int_classes
from os import listdir
from os.path import isfile, join
import torch.optim as optim
import torch.nn as nn
import torch.nn.functional as fc

# #####################################################################
# Separation - base
#
class Net(nn.Module):

    def __init__(self):
        super(Net, self).__init__()
        self.conv1 = nn.Conv1d(1, 256, 20)
        self.deconv = nn.ConvTranspose1d(256, 1, 20)

    def forward(self, input_data):
        input_data = self.conv1(input_data);
        input_data = fc.relu(input_data);
        input_data = self.deconv(input_data)
        return input_data

###################################################################
"""
AudioDataset
"""
class AudioDataset(data_utils.Dataset):
    """
    Dataset of speech mixtures for speech separation. 
    """
    def __init__(self, path, transform=None):
        super(AudioDataset, self).__init__()
        self.path = path
        self.mixtures_path = self.path + "mix/"
        self.source1_path  = self.path + "s1/"
        self.source2_path  = self.path + "s2/"
        self.mixtures = [f for f in listdir(self.mixtures_path) if isfile(join(self.mixtures_path, f))]
        self.transform = transform
        self.generator = self.generate_segment()

        self.current_mixture = ""
        self.audioindex = 0
        self.loadNextAudio()

    def __len__(self):
        return len(self.current_mixture)


    def __getitem__(self, index):
        """
        v2: transformovane a nachystane audio, ale pouze jeden segment v podobe tensoru
        """
        #print("f: __getitem__")
        segment = self.getSegment(self.current_mixture) # tato bude, misto return, mit yield
        return segment

    def loadNextAudio(self):
        print("f: load_next : " + self.mixtures[self.audioindex])
        print(str(self.audioindex) + " <?= " + str(len(self.mixtures)))
        self.current_mixture = self.getAudioSamples(self.mixtures_path + self.mixtures[self.audioindex])
        #normalizace na 0 mean a unit variance
        #self.current_mixture = norm(self.current_mixture)
        self.audioindex += 1
        self.generator = self.generate_segment() # reinitialisation of generator
        #self.i = 0 # segment step counter
        if self.audioindex > len(self.mixtures):
            print(str(self.audioindex) + " > " + str(len(self.mixtures)))
            return

    def getAudioSamples(self, audio_file_path):
        """
        Vrati vzorky zadaneho audio souboru
        """
        rate, samples = wav.read(audio_file_path)
        #print("f: get_audio_samples")
        #print(samples[0:19])
        #print(samples[20:39])
        return samples 

    def prepare(self, samples):
        # normalisation - zero mean & jednotkova variance (unit variation)
        numpy = np.array(samples)
        tensor = torch.as_tensor(numpy)
        return torch.tensor(tensor, dtype=torch.float32)
    #jeste normalizace TODO !!!

    def getSegment(self, path):
        #print("f: get_segment")
        next_segment = next(self.generator)
        return self.prepare(next_segment)

    def generate_segment(self):
        #print("f: generate_segments")
        samples = self.current_mixture
        segment = []

        for self.i in range(0, len(samples), 20):
            if self.i+20 > len(samples):
                break # kontrola, jestli i neni mimo pole
            #print("index i: " + str(self.i))
            segment = [samples[self.i] for self.i in range(self.i, self.i+20)]
            yield [segment]

        self.loadNextAudio() # uz neni co nacitat

####################################################

def norm(input_data):
    #if self.transform:
    #    self.current_mixture = self.transform(self.current_mixture)  
    tmp=np.array(input_data)
    d=tmp/sum(input_data)
    return d
#print(input_data)
    #s = sum(input_data)
    #normalized = [float(i)/s for i in input_data]
    #return normalized


#######################################################################

autoencoderNN = Net()
print(autoencoderNN)

#######################################################################
# --- testing ---
#input_data = torch.randn(10, 1, 20)
#output = autoencoderNN(input_data) 
#print(output)

#target = torch.randn(10, 1, 20)  # a dummy target, for example
#criterion = nn.MSELoss()
#loss = criterion(output, target)
#print(loss)

##########################################################################
#transform = transforms.Compose([
#    transforms.ToTensor(),
#    transforms.Normalize(mean=[0.456],
#                         std=[0.229])
#])

train_data_path = "/home/valgrut/Documents/full/min/tr/"
trainset = AudioDataset(train_data_path)
#print(len(trainset)) 

dataloader = data_utils.DataLoader(trainset, batch_size = 10, shuffle=False)
#####################################
#iterator = iter(dataloader)
#print("Cyklus:")
#for i in range (1, 10):
#minibatch = iterator.next()
#print(minibatch)

#########################################################################################

#criterion = nn.L1Loss()
#criterion = nn.MSELoss()

#criterion = nn.BCELoss()
criterion = nn.SmoothL1Loss()
#criterion = nn.KLDivLoss()

optimizer = optim.SGD(autoencoderNN.parameters(), lr=0.000001, momentum=0.5)

graph_x = []
graph_y = []
counter = 0

speech = []
for epoch_outer in range(5):
    for epoch in range(100):  # loop over the dataset multiple times
        running_loss = 0.0
        for cnt, data in enumerate(dataloader, 0):
            #print("training " + str(data.shape))
            # get the inputs
            #inputs, labels = data, data

            # ja mel celou dobu problem s tim, ze jsem porovnaval naprosto stejny veci????
            inputs = data
            targets = data.clone()

            inputs  = Variable(inputs.float())
            targets = Variable(targets.float())
            #targets = targets.long()


            # zero the parameter gradients
            optimizer.zero_grad()

            # forward + backward + optimize
            outputs = autoencoderNN(inputs)

            #speech.append([torch.Tensor.numpy(outputs[i])] for i in range(0, 9))
            #print(outputs.shape)
            lists = outputs.tolist()
            #print(lists[0][0])
            #print(lists[1][0])
            for elm in lists:
                speech += elm[0]
            #print()

            #targets = torch.squeeze(targets, 1) # uprava inputu [10, 1, 20] -> [10, 20]
            #outputs = torch.tensor(outputs)

            #print(outputs)
            #print(inputs)

            #norm_inputs  = norm(inputs)
            #norm_outputs = norm(outputs)
            #norm_targets = norm(targets)
            #loss = criterion(norm_outputs, norm_targets)

            loss = criterion(outputs, targets)
            loss.backward()
            optimizer.step()

            # print statistics
            running_loss += loss.item()
            if cnt % 100 == 99:
                print('[%d, %5d] loss: %.3f' % (epoch + 1, cnt + 1, running_loss / 2000))
                graph_x.append(counter)
                graph_y.append(running_loss/2000)
                counter += 1
                running_loss = 0.0

                #save output file
                speech_prep = np.array(speech)

                #print(speech_prep[0:15])
                wav.write("reconstruction/speech_"+str(epoch), 8000, speech_prep)
                speech = []



print('Finished Training')

plt.plot(graph_x, graph_y)        
plt.show()

exit(0)

