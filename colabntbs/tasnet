{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"tasnet","provenance":[],"collapsed_sections":[],"toc_visible":true},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"_xuWGpScqQH8","colab_type":"text"},"source":["# TasNet Bakalarka\n","- install pytorch\n","- check that cuda exists\n","- update repository in Google Drive\n","  - load data from Google Drive - Use Collab modified code\n","  - save reconstruction back to Google Drive\n","- Import libs - torchvision, scipy, etc.\n","- train nn\n","- save graph and hyper parameters into file, to know the best NN configuration"]},{"cell_type":"markdown","metadata":{"id":"PfQ01bj7SN0z","colab_type":"text"},"source":["## TODO\n","- save this notebook to github repo after finished work\n","- load this notebook from github before continuing work"]},{"cell_type":"markdown","metadata":{"id":"X8pnng8uOf3S","colab_type":"text"},"source":["## Prepare Google Drive with data"]},{"cell_type":"code","metadata":{"colab_type":"code","outputId":"08ffc48d-c599-43fc-edc1-bdc159459a52","executionInfo":{"status":"ok","timestamp":1573059662768,"user_tz":-60,"elapsed":24731,"user":{"displayName":"narutokov","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mAyJj_235BHAe-AhCcabJ67rI8ms6VvOAQiExuQ=s64","userId":"09699004457365288649"}},"id":"75e2X4bWZrRS","colab":{"base_uri":"https://localhost:8080/","height":124}},"source":["from google.colab import drive\n","drive.mount('/gdrive')"],"execution_count":7,"outputs":[{"output_type":"stream","text":["Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n","\n","Enter your authorization code:\n","··········\n","Mounted at /gdrive\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"s65oM83HZ7tw","colab_type":"text"},"source":["## Load Single file from github"]},{"cell_type":"code","metadata":{"id":"ixkljqm4fvQm","colab_type":"code","colab":{}},"source":["# Fetch a single <1MB file using the raw GitHub URL.\n","!curl --remote-name \\\n","     -H 'Accept: application/vnd.github.v3.raw' \\\n","     --location https://api.github.com/repos/valgrut/TasNet/colab/tasnet.ipynb"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"b0mwmOVJc28W","colab_type":"text"},"source":["## Save notebooks to my repo on github"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"BW-ZErjaicZi","colab":{}},"source":["GIT_USERNAME = \"valgrut\"\n","GIT_TOKEN = \"XXX\"\n","GIT_REPOSITORY = \"TasNet\"\n","\n","PROJECT_PATH = r\"/gdrive/My Drive/FIT/\"\n","\n","!mkdir ./temp\n","!git clone \"https://{GIT_TOKEN}@github.com/{GIT_USERNAME}/{GIT_REPOSITORY}.git\" ./temp\n","#!rsync -aP --exclude=data/ \"{PROJECT_PATH}\"/* ./temp\n","\n","%cd ./temp\n","!git add .\n","!git commit -m '\"New Version of TasNet\"'\n","!git config --global user.email \"naruto987@seznam.cz\"\n","!git config --global user.name \"valgrut\"\n","!git push origin \"master\"\n","%cd ../\n","!rm -rf ./temp"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"wKFxIQTBktqt","colab_type":"text"},"source":["## Save notebooks to tasnet repo on local\n","DOESNT WORK NOW\n"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"2mP-KYneo9ta","colab":{}},"source":["from google.colab import files\n","from google.colab import drive\n","import zipfile\n","import sys\n","drive.mount('/gdrive')\n","\n","!ls \"/gdrive/My Drive/Colab Notebooks/\"\n","\n","%cd \"/gdrive/My Drive/Colab Notebooks/\"\n","!ls\n","#files.download('/gdrive/My Drive/Colab Notebooks/tasnet.ipynb')\n","\n","\n","foldername = 'colabnotebooks'\n","zipfile.ZipFile('/gdrive/My Drive/'+foldername + '.zip', 'w', zipfile.ZIP_DEFLATED)\n","\n","#Downloading the data from the colab\n","\n","from google.colab import files\n","files.download('/gdrive/My Drive/test.zip')\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"j2bMCh9efEw8","colab_type":"text"},"source":["# Libs, params, classes definition"]},{"cell_type":"markdown","metadata":{"id":"BNU3SHR5Nlu1","colab_type":"text"},"source":["## Import libs\n"]},{"cell_type":"code","metadata":{"id":"5_0Q1MeKNYCK","colab_type":"code","colab":{}},"source":["import tkinter\n","from datetime import datetime\n","from scipy.io import wavfile as wav\n","import matplotlib.pyplot as plt\n","from torch.autograd import Variable\n","import torchvision.transforms as transforms\n","import IPython.display as ipd\n","import numpy as np\n","import torch\n","import torch.utils.data as data_utils\n","from torch._six import int_classes as _int_classes\n","import signal\n","import sys\n","from os import listdir\n","from os.path import isfile, join\n","import torch.optim as optim\n","import torch.nn as nn\n","import torch.nn.functional as fc"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"9zIPLH3ZNhbg","colab_type":"text"},"source":["## Hyperparameters for NN"]},{"cell_type":"code","metadata":{"id":"UMZ1M3aQNgL2","colab_type":"code","colab":{}},"source":["### HYPERPARAMETERS ###\n","DEBUG = False\n","\n","BASE_DATA_PATH=r\"/gdrive/My Drive/FIT/\"\n","\n","MINIBATCH_SIZE  = 1       # TODO Problem s rozmerama pri hodnote > 1\n","\n","optim_SGD       = False   # Adam / SGD\n","learning_rate   = 0.0001\n","opt_decay       = 0       # 0.0001\n","\n","bias_enabled    = False\n","padd            = 10       # 20 nebo 10?? (parametry nahovno: 20, lr=0,0001)\n","nn_stride       = 20\n","\n","use_cuda        = True\n","audio_save_rate = 10000\n","epochs          = 2\n","audios_in_epoch = 20000 # kolik zpracovat nahravek v jedne epose\n","print_frequency = 10000 # za kolik segmentu (minibatchu) vypisovat loss\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"LfsnpnXuNrvt","colab_type":"text"},"source":["## ResBlock class"]},{"cell_type":"code","metadata":{"id":"-BUAJ6lzN1w2","colab_type":"code","colab":{}},"source":["class ResBlock(nn.Module):\n","    def __init__(self, in_channels, dilation):\n","        super(ResBlock, self).__init__()\n","        self.dilation = dilation\n","\n","        self.conv1 = nn.Conv1d(256, 512, kernel_size=1)\n","        self.D_conv = nn.Conv1d(512, 512, kernel_size=3, padding=self.dilation, groups=512, dilation=self.dilation)\n","        self.conv2 = nn.Conv1d(512, 256, kernel_size=1)\n","\n","        self.batch1 = nn.BatchNorm1d(512)\n","        self.batch2 = nn.BatchNorm1d(512)\n","\n","        self.prelu1 = nn.PReLU(512)\n","        self.prelu2 = nn.PReLU(512)\n","\n","    def forward(self, input_data):\n","        if DEBUG: \n","            print(\"ResBlock Start: shape of input_data:\", input_data.shape)\n","        x = self.conv1(input_data)\n","        x = self.prelu1(x)\n","        x = self.batch1(x)\n","        x = self.D_conv(x)\n","        if DEBUG:\n","            print(\"ResBlock middle shape:\", x.shape)\n","        #x = torch.reshape(x, (1, -1,))\n","        #x = torch.reshape(x, (-1,))\n","        if DEBUG:\n","            print(\"ResBlock po concatenaci:\", x.shape)\n","        x = self.prelu2(x)\n","        x = self.batch2(x)\n","        x = self.conv2(x)\n","        if DEBUG:\n","            print(\"ResBlock after conv2: \", x.shape)\n","            print(\"ResBlock end: input_data: \", input_data.shape)\n","        return torch.add(x, input_data)\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"1KMP5GHiN2MZ","colab_type":"text"},"source":["## Main NeuralNetwork Class\n"]},{"cell_type":"code","metadata":{"id":"6R_7Hpj0N2ls","colab_type":"code","colab":{}},"source":["class Net(nn.Module):\n","    def __init__(self):\n","        super(Net, self).__init__()\n","        self.conv1 = nn.Conv1d(1, 256, 20, bias=bias_enabled, stride=nn_stride, padding=padd)\n","        #self.deconv = nn.ConvTranspose1d(256, 2, 20, padding=padd, bias=bias_enabled, stride=20)\n","        self.deconv = nn.ConvTranspose1d(512, 2, 20, padding=padd, bias=bias_enabled, stride=nn_stride, groups=2)\n","\n","        self.layer_norm = nn.LayerNorm(256)\n","        self.bottleneck1 = nn.Conv1d(256, 256, 1) #TODO padding, stride???\n","        self.bottleneck2 = nn.Conv1d(256, 512, 1) #TODO 512 = NxC\n","        self.softmax = nn.Softmax(2)\n","\n","        self.resblock1 = ResBlock(256, 1)\n","        self.resblock2 = ResBlock(256, 2)\n","        self.resblock3 = ResBlock(256, 4)\n","        self.resblock4 = ResBlock(256, 8)\n","        self.resblock5 = ResBlock(256, 16)\n","        self.resblock6 = ResBlock(256, 32)\n","        self.resblock7 = ResBlock(256, 64)\n","\n","        self.resblock11 = ResBlock(256, 1)\n","        self.resblock12 = ResBlock(256, 2)\n","        self.resblock13 = ResBlock(256, 4)\n","        self.resblock14 = ResBlock(256, 8)\n","        self.resblock15 = ResBlock(256, 16)\n","        self.resblock16 = ResBlock(256, 32)\n","        self.resblock17 = ResBlock(256, 64)\n","        # TODO je tohle OK?\n","        torch.nn.init.xavier_uniform_(self.conv1.weight)\n","        torch.nn.init.xavier_uniform_(self.deconv.weight)\n","\n","    def forward(self, input_data):\n","        # encoder\n","        if DEBUG:\n","            print(\"Net start - create representation from input_data: \", input_data.shape)\n","        representation = self.conv1(input_data)\n","        representation = fc.relu(representation)\n","        if DEBUG:    \n","            print(\"Net: advanced representation created\", representation.shape)\n","\n","        # separation - estimate masks\n","        #representation = self.layer_norm(representation) # TODO -layer norm\n","        data = self.bottleneck1(representation)\n","\n","        data = self.resblock1(data)\n","        data = self.resblock2(data)\n","        data = self.resblock3(data)\n","        data = self.resblock4(data)\n","        data = self.resblock5(data)\n","        data = self.resblock6(data)\n","        data = self.resblock7(data)\n","\n","        data = self.resblock11(data)\n","        data = self.resblock12(data)\n","        data = self.resblock13(data)\n","        data = self.resblock14(data)\n","        data = self.resblock15(data)\n","        data = self.resblock16(data)\n","        data = self.resblock17(data)\n","\n","        data = self.bottleneck2(data)\n","        data = torch.reshape(data, (1, 256, 2, -1,))\n","        masks = self.softmax(data)\n","        if DEBUG:\n","            print(\"NN: Masks: \", masks.shape)\n","\n","        # multiply masks and representation\n","        masked_representation = torch.mul(representation[:,:,None,:], masks)\n","        masked_representation = torch.reshape(masked_representation, (1, 512, -1))\n","        \n","        # decoder\n","        separate_data = self.deconv(masked_representation)\n","        return separate_data"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"DgOHc1upOFy-","colab_type":"text"},"source":["## Data Loader Class"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"T6n7_lS9NWeI","colab":{}},"source":["class AudioDataset(data_utils.Dataset):\n","    \"\"\"\n","    Dataset of speech mixtures for speech separation.\n","    \"\"\"\n","    def __init__(self, path, transform=None):\n","        super(AudioDataset, self).__init__()\n","        self.path = path\n","        self.mixtures_path = self.path + \"mix/\"\n","        self.sources1_path  = self.path + \"s1/\"\n","        self.sources2_path  = self.path + \"s2/\"\n","\n","        self.mixtures = []\n","        self.sources1 = []\n","        self.sources2 = []\n","\n","        # self.mixtures je vektor, kde jsou ulozeny nazvy vsech audio nahravek urcenych k uceni site.\n","        self.mixtures = [mix for mix in listdir(self.mixtures_path) if isfile(join(self.mixtures_path, mix))]\n","        self.sources1 = [s1 for s1 in listdir(self.sources1_path) if isfile(join(self.sources1_path, s1))]\n","        self.sources2 = [s2 for s2 in listdir(self.sources2_path) if isfile(join(self.sources2_path, s2))]\n","\n","        # ono by vlastne stacilo rozkopirovat unique mixture do zbylych dvou sources1 a sources2 misto tech zbylych rozdilu.\n","        # V obou totiz ma byt to same a ve stejnem poctu.\n","\n","        # REMOVE DUPLICATES\n","        if DEBUG:\n","            print(\"audiodataset mixture size: \", len(self.mixtures))\n","            print(\"audiodataset sources1 size: \", len(self.sources1))\n","            print(\"audiodataset sources1 size: \", len(self.sources2))\n","        # make list unique\n","        smixtures = set(self.mixtures)\n","        ssources1 = set(self.sources1)\n","        ssources2 = set(self.sources2)\n","\n","        ms1_duplicates = smixtures - ssources1\n","        ms2_duplicates = smixtures - ssources2\n","        self.mixtures = list((smixtures - ms1_duplicates) - ms2_duplicates)\n","\n","        s1m_duplicates = ssources1 - smixtures\n","        s2m_duplicates = ssources2 - smixtures\n","        self.sources1 = list(((ssources1 - s1m_duplicates) - s2m_duplicates) - ms2_duplicates)\n","        self.sources2 = list(((ssources2 - s2m_duplicates) - s1m_duplicates) - ms1_duplicates)\n","\n","        self.mixtures.sort()\n","        self.sources1.sort()\n","        self.sources2.sort()\n","\n","\n","        if DEBUG:\n","            print(\"audiodataset mixture size: \", len(self.mixtures))\n","            print(\"audiodataset sources1 size: \", len(self.sources1))\n","            print(\"audiodataset sources1 size: \", len(self.sources2))\n","\n","    def __len__(self):\n","        \"\"\"\n","        Vraci celkovy pocet dat, ktere jsou zpracovavane\n","        \"\"\"\n","        return len(self.mixtures)\n","\n","    def __getitem__(self, index):\n","        \"\"\"\n","        v2: transformovane a nachystane audio, ale pouze jeden segment v podobe tensoru\n","        \"\"\"\n","        if DEBUG:\n","            print(\"getItem: index:\",index, \" path: \", self.mixtures_path + self.mixtures[index])\n","            print(\"getItem: index:\",index, \" path: \", self.sources1_path + self.sources1[index])\n","            print(\"getItem: index:\",index, \" path: \", self.sources2_path + self.sources2[index])\n","        mixture = self.getAudioSamples(self.mixtures_path + self.mixtures[index])\n","        source1 = self.getAudioSamples(self.sources1_path + self.sources1[index])\n","        source2 = self.getAudioSamples(self.sources2_path + self.sources2[index])\n","        mixture.unsqueeze_(0)\n","        source1.unsqueeze_(0)\n","        source2.unsqueeze_(0)\n","        return mixture, source1, source2\n","\n","    def getAudioSamples(self, audio_file_path):\n","        \"\"\"\n","        Precte a vrati vsechny vzorky zadaneho audio souboru\n","        \"\"\"\n","        rate, samples = wav.read(audio_file_path)\n","        return self.prepare(samples)\n","\n","    def prepare(self, samples):\n","        \"\"\"\n","        Funkce prevede vstupni vzorky na numpy array a nasledne na tensor.\n","        \"\"\"\n","        # normalisation - zero mean & jednotkova variance (unit variation)\n","        numpy = np.array(samples)\n","        #normalizace\n","        numpy = numpy / 2**15\n","        tensor = torch.as_tensor(numpy)\n","        tensor_float32 = torch.tensor(tensor, dtype=torch.float32)\n","        return tensor_float32"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"1Ne4JZ_7PVZ3","colab_type":"text"},"source":["# Main - Training and using NN\n","This have to be run +-3 times to make nn run. Something with too large dataset error... "]},{"cell_type":"markdown","metadata":{"id":"b5zI3nXegLWK","colab_type":"text"},"source":["## 1. Instantiate NN"]},{"cell_type":"code","metadata":{"id":"mLD1RfLOgQEh","colab_type":"code","colab":{}},"source":["tasnet = Net()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"bxOYp1IJgfh1","colab_type":"text"},"source":["## 2. Use Cuda if available\n","To enable GPU backend for your notebook, go to Edit → Notebook Settings and set Hardware accelerator to GPU.\n"]},{"cell_type":"code","metadata":{"id":"5bAVMAMWkqAT","colab_type":"code","outputId":"44ae7248-98fe-4540-8e45-ac8a29c9d8ec","executionInfo":{"status":"ok","timestamp":1573059683951,"user_tz":-60,"elapsed":6354,"user":{"displayName":"narutokov","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mAyJj_235BHAe-AhCcabJ67rI8ms6VvOAQiExuQ=s64","userId":"09699004457365288649"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["# Check if cuda is available\n","if use_cuda and torch.cuda.is_available():\n","  print(\"Cuda is available!\")\n","  tasnet.cuda()\n","else:\n","  print(\"Cuda is NOT available\")"],"execution_count":10,"outputs":[{"output_type":"stream","text":["Cuda is available!\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"JGl20A5Egz3p","colab_type":"text"},"source":["## 3. Work With Network\n"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"NAqfZOBG2DT8"},"source":["### TODO\n","- [opt] Load from git\n","- [opt] save to git\n","- [opt] zkontrolovat jestli je vse prevedeno na cuda v pohode.\n","- [opt] Zkontrolovat jestli jde nacist sit z checkpointu (GPU/CPU)\n","- **vysetrit error ktery mi to vyhodilo rano: Index Out of bound**\n","- **plot realtime graph of training loss**"]},{"cell_type":"markdown","metadata":{"id":"ezZ6t7qa0CgF","colab_type":"text"},"source":["## Set Criterion and Optimizer"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"HLkZhYsx1_Ul","colab":{}},"source":["criterion = nn.MSELoss()\n","optimizer = optim.Adam(tasnet.parameters(), lr = learning_rate, weight_decay=opt_decay)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"1GRhQ-b34PzE","colab_type":"text"},"source":["## [opt] Load NN from Checkpoint "]},{"cell_type":"markdown","metadata":{"id":"b_hDJqwzxU5z","colab_type":"text"},"source":["### [opt] Load network for Inference (Production)\n","https://pytorch.org/tutorials/beginner/saving_loading_models.html"]},{"cell_type":"code","metadata":{"id":"GKHwMr3uxYKX","colab_type":"code","colab":{}},"source":["tasnet.load_state_dict(torch.load(BASE_DATA_PATH+'tasnet_model_inference.pkl'))\n","tasnet.eval()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"4m0cm7wg1NKr","colab_type":"text"},"source":["### [opt] Load Checkpoint for continuing training"]},{"cell_type":"code","metadata":{"id":"4rBwoxHa1VFK","colab_type":"code","colab":{}},"source":["#replace Y,M,D,H,m,X,Y  with apropriate values\n","checkpoint = torch.load(BASE_DATA_PATH+'tasnet_model_checkpoint_Y-M-D_H:m_eX_aY.tar')\n","tasnet.load_state_dict(checkpoint['model_state_dict'])\n","optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n","epoch = checkpoint['epoch']\n","loss = checkpoint['loss']\n","\n","# tasnet.eval() # For inference\n","# - or - continue training:\n","tasnet.train()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"C_79odPkzcBF","colab_type":"text"},"source":["## 3. a) Training NN"]},{"cell_type":"code","metadata":{"colab_type":"code","outputId":"25106fea-7f50-4dc1-8a74-89cfff2cf013","executionInfo":{"status":"error","timestamp":1573065012294,"user_tz":-60,"elapsed":38156,"user":{"displayName":"narutokov","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mAyJj_235BHAe-AhCcabJ67rI8ms6VvOAQiExuQ=s64","userId":"09699004457365288649"}},"id":"MyR1iWtvg8Q6","colab":{"base_uri":"https://localhost:8080/","height":732}},"source":["train_data_path = BASE_DATA_PATH+\"dataset/tr/\"\n","valid_data_path = BASE_DATA_PATH+\"dataset/cv/\"\n","\n","trainset = AudioDataset(train_data_path)\n","validset = AudioDataset(valid_data_path)\n","\n","# Note: We shuffle the loading process of train_dataset to make the learning process \n","# independent of data order, but the order of test_loader \n","# remains so as to examine whether we can handle unspecified bias order of inputs.\n","trainloader = data_utils.DataLoader(trainset, batch_size = MINIBATCH_SIZE, shuffle=True)\n","validloader = data_utils.DataLoader(validset, batch_size = MINIBATCH_SIZE, shuffle=False)\n","\n","best_validation_result = 42   #initial value\n","graph_x = []\n","graph_y = []\n","\n","global_audio_cnt = 0\n","for epoch in range(epochs):\n","    running_loss = 0.0\n","\n","    # tady by data byly dvojice puvodni delky a te nahravky\n","    for audio_cnt, data in enumerate(trainloader, 0):\n","        if audio_cnt > audios_in_epoch:\n","            break #TODO pak oddelat\n","\n","        global_audio_cnt += 1\n","        #if audio_cnt % 100 == 0:\n","        print(datetime.now().strftime('%Y-%m-%d %H:%M:%S'), epoch, audio_cnt)\n","\n","        input_mixture  = data[0]\n","        target_source1 = data[1]\n","        target_source2 = data[2]\n","\n","        # TODO check if OK\n","        if use_cuda and torch.cuda.is_available():\n","            input_mixture = input_mixture.cuda()\n","            target_source1 = target_source1.cuda()\n","            target_source2 = target_source2.cuda()\n","\n","        optimizer.zero_grad()\n","        separated_sources = tasnet(input_mixture)\n","\n","        #print(outputs.shape, target.shape)\n","        #if target.shape[2] != outputs.shape[2]:\n","        #    target = target.narrow(2, 0, outputs.shape[2])\n","\n","        ## zkraceni nahravek tak, aby vsechny byly stejne dlouho - pocet samplu stejny\n","        smallest = min(input_mixture.shape[2], target_source1.shape[2], target_source2.shape[2], separated_sources.shape[2])\n","        input_mixture = input_mixture.narrow(2, 0, smallest)\n","        target_source1 = target_source1.narrow(2, 0, smallest)\n","        target_source2 = target_source2.narrow(2, 0, smallest)\n","        separated_sources = separated_sources.narrow(2, 0, smallest)\n","\n","        # spojeni sources do jedne matice\n","        target_sources = torch.cat((target_source1, target_source2), 1)\n","\n","        #loss = criterion(separated_sources, target_sources)\n","        #loss.backward()\n","        #optimizer.step()\n","\n","        # calculate average loss\n","        running_loss += loss.item()\n","        if audio_cnt % print_frequency == print_frequency-1:\n","            print('[%d, %5d] loss: %.5f' % (epoch, audio_cnt, running_loss/print_frequency))\n","            #graph_x.append(epoch) #TODO\n","            graph_x.append(print_frequency)\n","            graph_y.append(running_loss/print_frequency)\n","            running_loss = 0.0\n","\n","            # Create snapshot - checkpoint\n","            torch.save({\n","              'epoch': epoch,\n","              'audio_cnt': audio_cnt,\n","              'model_state_dict': tasnet.state_dict(),\n","              'optimizer_state_dict': optimizer.state_dict(),\n","              'loss': loss,\n","            }, BASE_DATA_PATH+'tasnet_model_checkpoint_'+str(datetime.now().strftime('%Y-%m-%d_%H:%M'))+'_e'+str(epoch)+'_a'+str(audio_cnt)+'.tar')\n","\n","        # ulozeni pouze prvni nahravky pro porovnani epoch\n","        #if audio_cnt == 0:\n","        # ulozit kazdou Xtou rekonstrukci pro moznost jejiho prehrati a zjisteni, jak to zni.\n","        if audio_cnt % audio_save_rate == 0:\n","            mixture_prep = 0\n","            source1_prep = 0\n","            source2_prep = 0\n","\n","            if use_cuda and torch.cuda.is_available():\n","                mixture_prep = input_mixture.cpu().detach().numpy()\n","                source1_prep = separated_sources[0][0].cpu().detach().numpy()\n","                source2_prep = separated_sources[0][1].cpu().detach().numpy()\n","            else:\n","                mixture_prep = input_mixture.detach().numpy()\n","                source1_prep = separated_sources[0][0].detach().numpy()\n","                source2_prep = separated_sources[0][1].detach().numpy()\n","\n","            wav.write(BASE_DATA_PATH+\"reconstruction/speech_e\"+str(epoch)+\"_a\"+str(audio_cnt)+\"_s1.wav\", 8000, source1_prep)\n","            wav.write(BASE_DATA_PATH+\"reconstruction/speech_e\"+str(epoch)+\"_a\"+str(audio_cnt)+\"_s2.wav\", 8000, source2_prep)\n","            wav.write(BASE_DATA_PATH+\"reconstruction/speech_e\"+str(epoch)+\"_a\"+str(audio_cnt)+\"_mix.wav\", 8000, mixture_prep)\n","\n","    # === validation na konci epochy ===\n","    print(\"\")\n","    print(\"Validace\")\n","    valid_audio_cnt = 0\n","    running_loss = 0.0\n","    current_validation_result = 0\n","\n","    for audio_cnt, data in enumerate(validloader, 0):\n","        if valid_audio_cnt > 500:\n","            break #TODO pak oddelat\n","        valid_audio_cnt += 1\n","\n","        input_mixture  = data[0]\n","        target_source1 = data[1]\n","        target_source2 = data[2]\n","\n","        if use_cuda and torch.cuda.is_available():\n","            input_mixture = input_mixture.cuda()\n","            target_source1 = target_source1.cuda()\n","            target_source2 = target_source2.cuda()\n","\n","        optimizer.zero_grad()\n","        separated_sources = tasnet(input_mixture)\n","\n","        smallest = min(input_mixture.shape[2], target_source1.shape[2], target_source2.shape[2], separated_sources.shape[2])\n","        input_mixture = input_mixture.narrow(2, 0, smallest)\n","        target_source1 = target_source1.narrow(2, 0, smallest)\n","        target_source2 = target_source2.narrow(2, 0, smallest)\n","        separated_sources = separated_sources.narrow(2, 0, smallest)\n","\n","        # spojeni sources do jedne matice\n","        target_sources = torch.cat((target_source1, target_source2), 1)\n","\n","        loss = criterion(separated_sources, target_sources)\n","\n","        current_validation_result += loss.item()\n","        running_loss += loss.item()\n","        if audio_cnt % print_frequency == print_frequency-1:\n","            print('[%5d] loss: %.4f' % (audio_cnt+1, running_loss/print_frequency))\n","            running_loss = 0.0\n","\n","\n","    # vyhodnoceni validace\n","    current_validation_result /= valid_audio_cnt # prumer\n","    print(current_validation_result, \" \", best_validation_result)\n","    if current_validation_result >= best_validation_result:\n","        learning_rate /= 2\n","    else:\n","        best_validation_result = current_validation_result\n","    print('Finished Validating')\n","    print('')\n","\n","\n","# Save Network For Inference in the end of training\n","torch.save(tasnet.state_dict(), BASE_DATA_PATH+'tasnet_model_inference.pkl')\n","print('Finished Training')\n","\n","plt.plot(graph_x, graph_y)\n","plt.show()\n"],"execution_count":51,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:91: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"],"name":"stderr"},{"output_type":"stream","text":["2019-11-06 18:29:06 0 0\n","2019-11-06 18:29:08 0 1\n","2019-11-06 18:29:09 0 2\n","2019-11-06 18:29:10 0 3\n","2019-11-06 18:29:11 0 4\n","2019-11-06 18:29:12 0 5\n","2019-11-06 18:29:13 0 6\n","2019-11-06 18:29:14 0 7\n","2019-11-06 18:29:15 0 8\n","2019-11-06 18:29:16 0 9\n","2019-11-06 18:29:17 0 10\n","2019-11-06 18:29:18 0 11\n","2019-11-06 18:29:19 0 12\n","2019-11-06 18:29:20 0 13\n","2019-11-06 18:29:22 0 14\n","2019-11-06 18:29:23 0 15\n","2019-11-06 18:29:24 0 16\n","2019-11-06 18:29:25 0 17\n"],"name":"stdout"},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-51-066dff7c5633>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0;31m# tady by data byly dvojice puvodni delky a te nahravky\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0maudio_cnt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0maudio_cnt\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0maudios_in_epoch\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m             \u001b[0;32mbreak\u001b[0m \u001b[0;31m#TODO pak oddelat\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    344\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__next__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    345\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 346\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    347\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    348\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-50-d0ff2d98a1ba>\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m     66\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"getItem: index:\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\" path: \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msources2_path\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msources2\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m         \u001b[0mmixture\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetAudioSamples\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmixtures_path\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmixtures\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 68\u001b[0;31m         \u001b[0msource1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetAudioSamples\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msources1_path\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msources1\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     69\u001b[0m         \u001b[0msource2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetAudioSamples\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msources2_path\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msources2\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m         \u001b[0mmixture\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-50-d0ff2d98a1ba>\u001b[0m in \u001b[0;36mgetAudioSamples\u001b[0;34m(self, audio_file_path)\u001b[0m\n\u001b[1;32m     77\u001b[0m         \u001b[0mPrecte\u001b[0m \u001b[0ma\u001b[0m \u001b[0mvrati\u001b[0m \u001b[0mvsechny\u001b[0m \u001b[0mvzorky\u001b[0m \u001b[0mzadaneho\u001b[0m \u001b[0maudio\u001b[0m \u001b[0msouboru\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m         \"\"\"\n\u001b[0;32m---> 79\u001b[0;31m         \u001b[0mrate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msamples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwav\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maudio_file_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     80\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprepare\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msamples\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/scipy/io/wavfile.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(filename, mmap)\u001b[0m\n\u001b[1;32m    234\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    235\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 236\u001b[0;31m         \u001b[0mfile_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_big_endian\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_read_riff_chunk\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    237\u001b[0m         \u001b[0mfmt_chunk_received\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    238\u001b[0m         \u001b[0mchannels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/scipy/io/wavfile.py\u001b[0m in \u001b[0;36m_read_riff_chunk\u001b[0;34m(fid)\u001b[0m\n\u001b[1;32m    156\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    157\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_read_riff_chunk\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 158\u001b[0;31m     \u001b[0mstr1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfid\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# File signature\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    159\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mstr1\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34mb'RIFF'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    160\u001b[0m         \u001b[0mis_big_endian\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"cell_type":"markdown","metadata":{"id":"S8zzAG302t47","colab_type":"text"},"source":["## b) Test Network"]},{"cell_type":"code","metadata":{"id":"KXDQAxJz2wcn","colab_type":"code","colab":{}},"source":["# Load Test dataset\n","test_data_path  = BASE_DATA_PATH+\"dataset/tt/\"\n","testset  = AudioDataset(test_data_path)\n","testloader  = data_utils.DataLoader(testset, batch_size = MINIBATCH_SIZE, shuffle=False)\n","\n","\n","# Start Testing\n","global_audio_cnt = 0\n","\n","for audio_cnt, source1 in enumerate(testloader, 0):\n","    global_audio_cnt += 1\n","    inputs = source1\n","    target = inputs.clone()\n","\n","    optimizer.zero_grad()\n","    outputs = tasnet(inputs)\n","\n","    if target.shape[2] != outputs.shape[2]:\n","        target = target.narrow(2, 0, outputs.shape[2])\n","        #print(\"Reshaped:\", outputs.shape, target.shape)\n","\n","    loss = criterion(outputs, target)\n","\n","    running_loss += loss.item()\n","    if audio_cnt % print_frequency == print_frequency-1:\n","        print('[%5d] loss: %.4f' % (audio_cnt+1, running_loss/print_frequency))\n","        running_loss = 0.0\n","\n","    speech_prep = outputs.detach().numpy()\n","    wav.write(BASE_DATA_PATH+\"testdata_recon/speech_a\"+str(audio_cnt)+\".wav\", 8000, speech_prep)\n","\n","print('Finished Testing')"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"umSfQ7OH24rB","colab_type":"text"},"source":["## c) Network Inference\n"]},{"cell_type":"code","metadata":{"id":"oCS1IYM729N9","colab_type":"code","colab":{}},"source":["print('Prepared for inference, load your audio.')"],"execution_count":0,"outputs":[]}]}