% ====================== Kapitola 1 =====================

% Separace mluvcich a koktejl party - prepinani pozornosti
@article{GETZMANN20171,
title = "Switching of auditory attention in “cocktail-party” listening: ERP evidence of cueing effects in younger and older adults",
journal = "Brain and Cognition",
volume = "111",
pages = "1 - 12",
year = "2017",
issn = "0278-2626",
doi = "https://doi.org/10.1016/j.bandc.2016.09.006",
url = "http://www.sciencedirect.com/science/article/pii/S0278262616302408",
author = "Stephan Getzmann and Julian Jasny and Michael Falkenstein",
keywords = "Aging, Speech perception, Attention, Cueing, Event-related potentials",
abstract = "Verbal communication in a “cocktail-party situation” is a major challenge for the auditory system. In particular, changes in target speaker usually result in declined speech perception. Here, we investigated whether speech cues indicating a subsequent change in target speaker reduce the costs of switching in younger and older adults. We employed event-related potential (ERP) measures and a speech perception task, in which sequences of short words were simultaneously presented by four speakers. Changes in target speaker were either unpredictable or semantically cued by a word within the target stream. Cued changes resulted in a less decreased performance than uncued changes in both age groups. The ERP analysis revealed shorter latencies in the change-related N400 and late positive complex (LPC) after cued changes, suggesting an acceleration in context updating and attention switching. Thus, both younger and older listeners used semantic cues to prepare changes in speaker setting."
}

% Separace uvod, popis cocktail party, dosavadni metody reseni separace
% https://link-springer-com.ezproxy.lib.vutbr.cz/article/10.1631/FITEE.1700814#citeas
%
%Qian, Y., Weng, C., Chang, X. et al. Past review, current progress, and challenges ahead on the %cocktail party problem. Frontiers Inf Technol Electronic Eng 19, 40–63 (2018). https://doi-%org.ezproxy.lib.vutbr.cz/10.1631/FITEE.1700814
%Received08 December 2017
%Revised17 January 2018
%Published25 January 2018
%Issue DateJanuary 2018
@article{cocktailparty,
  title={Past review, current progress, and challenges ahead on the cocktail party problem},
  author={Qian, Y., Weng, C., Chang, X. et al.},
  journal={Frontiers Inf Technol Electronic Eng},
  volume={19},
  number={1},
  pages={40--63},
  year={2018},
  url={https://doi-org.ezproxy.lib.vutbr.cz/10.1631/FITEE.1700814}
}

% CASA ze studie
% https://pdfs.semanticscholar.org/8875/39061b4dec2439180eb58c2d6eae13f24548.pdf
@article{choi2005blind,
  title={Blind source separation and independent component analysis: A review},
  author={Choi, Seungjin and Cichocki, Andrzej and Park, Hyung-Min and Lee, Soo-Young},
  journal={Neural Information Processing-Letters and Reviews},
  volume={6},
  number={1},
  pages={1--57},
  year={2005}
}


% NMF
% https://papers.nips.cc/paper/1861-algorithms-for-non-negative-matrix-factorization
% http://papers.nips.cc/paper/1861-algorithms-for-non-negative-matrix-factorization.pdf
@incollection{NIPS2000_1861,
title = {Algorithms for Non-negative Matrix Factorization},
author = {Daniel D. Lee and Seung, H. Sebastian},
booktitle = {Advances in Neural Information Processing Systems 13},
editor = {T. K. Leen and T. G. Dietterich and V. Tresp},
pages = {556--562},
year = {2001},
publisher = {MIT Press},
url = {http://papers.nips.cc/paper/1861-algorithms-for-non-negative-matrix-factorization.pdf}
}

% NMF z TasNet
% http://sap.ist.i.kyoto-u.ac.jp/members/yoshii/papers/ismir-2013-yoshii.pdf
@inproceedings{yoshii2013beyond,
  title={Beyond NMF: Time-Domain Audio Source Separation without Phase Reconstruction.},
  author={Yoshii, Kazuyoshi and Tomioka, Ryota and Mochihashi, Daichi and Goto, Masataka},
  booktitle={ISMIR},
  pages={369--374},
  year={2013}
}

% beamforming
% https://arxiv.org/abs/1212.6080
@misc{adel2012beamforming,
    title={Beamforming Techniques for Multichannel audio Signal Separation},
    author={Hidri Adel and Meddeb Souad and Abdulqadir Alaqeeli and Amiri Hamid},
    year={2012},
    eprint={1212.6080},
    archivePrefix={arXiv},
    primaryClass={cs.OH}
}

% 
% https://iopscience.iop.org/article/10.1088/1741-2560/13/2/026027/meta
@article{Negro_2016,
	doi = {10.1088/1741-2560/13/2/026027},
	url = {https://doi.org/10.1088%2F1741-2560%2F13%2F2%2F026027},
	year = 2016,
	month = {feb},
	publisher = {{IOP} Publishing},
	volume = {13},
	number = {2},
	pages = {026027},
	author = {Francesco Negro and Silvia Muceli and Anna Margherita Castronovo and Ales Holobar and Dario Farina},
	title = {Multi-channel intramuscular and surface {EMG} decomposition by convolutive blind source separation},
	journal = {Journal of Neural Engineering},
	abstract = {Objective. The study of motor unit behavior has been classically performed by selective recording systems of muscle electrical activity (EMG signals) and decomposition algorithms able to discriminate between individual motor unit action potentials from multi-unit signals. In this study, we provide a general framework for the decomposition of multi-channel intramuscular and surface EMG signals and we extensively validate this approach with experimental recordings. Approach. First, we describe the conditions under which the assumptions of the convolutive blind separation model are satisfied. Second, we propose an approach of convolutive sphering of the observations followed by an iterative extraction of the sources. This approach is then validated using intramuscular signals recorded by novel multi-channel thin-film electrodes on the Abductor Digiti Minimi of the hand and Tibilias Anterior muscles, as well as on high-density surface EMG signals recorded by electrode grids on the First Dorsal Interosseous muscle. The validation was based on the comparison with the gold standard of manual decomposition (for intramuscular recordings) and on the two-source method (for comparison of intramuscular and surface EMG recordings) for the three human muscles and contraction forces of up to 90% MVC. Main results. The average number of common sources identified for the validation was 14 ± 7 (averaged across all trials and subjects and all comparisons), with a rate of agreement in their discharge timings of 92.8 ± 3.2%. The average Decomposability Index, calculated on the automatic decomposed signals, was 16.0 ± 2.2 (7.3–44.1). For comparison, the same index calculated on the manual decomposed signals was 15.0 ± 3.0 (6.3–76.6). Significance. These results show that the method provides a solid framework for the decomposition of multi-channel invasive and non-invasive EMG signals that allows the study of the behavior of a large number of concurrently active motor units.}
}


@misc{luo2018convtasnet,
    title={Conv-TasNet: Surpassing Ideal Time-Frequency Magnitude Masking for Speech Separation},
    author={Yi Luo and Nima Mesgarani},
    year={2018},
    eprint={1809.07454},
    archivePrefix={arXiv},
    primaryClass={cs.SD}
}

% MSE, MAE, 
% https://towardsdatascience.com/understanding-the-3-most-common-loss-functions-for-machine-learning-regression-23e0ef3e14d3

% Obrazek FeedForward NN vs Recurrent NN
@article{FFandRecNN,
	author = {Pekel, Engin and Kara, Selin},
	year = {2017},
	month = {03},
	pages = {157-179},
	title = {A COMPREHENSIVE REVIEW FOR ARTIFICAL NEURAL NETWORK APPLICATION TO PUBLIC TRANSPORTATION},
	volume = {35},
	journal = {Sigma Journal of Engineering and Natural Sciences}
}

% Relu article citation about success of ReLU
@incollection{NIPS2012_4824,
title = {ImageNet Classification with Deep Convolutional Neural Networks},
author = {Alex Krizhevsky and Sutskever, Ilya and Hinton, Geoffrey E},
booktitle = {Advances in Neural Information Processing Systems 25},
editor = {F. Pereira and C. J. C. Burges and L. Bottou and K. Q. Weinberger},
pages = {1097--1105},
year = {2012},
publisher = {Curran Associates, Inc.},
url = {http://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf}
}

% aktivacni funkce -- sigmoidy, softmax, relu a pod
% https://arxiv.org/abs/1811.03378
@misc{nwankpa2018activation,
    title={Activation Functions: Comparison of trends in Practice and Research for Deep Learning},
    author={Chigozie Nwankpa and Winifred Ijomah and Anthony Gachagan and Stephen Marshall},
    year={2018},
    eprint={1811.03378},
    archivePrefix={arXiv},
    primaryClass={cs.LG}
}

%logistic sigmoid nevyhody a vyhody
%https://www.sciencedirect.com/science/article/abs/pii/0004370292900656?via%3Dihub
@article{NEAL199271,
title = "Connectionist learning of belief networks",
journal = "Artificial Intelligence",
volume = "56",
number = "1",
pages = "71 - 113",
year = "1992",
issn = "0004-3702",
doi = "https://doi.org/10.1016/0004-3702(92)90065-6",
url = "http://www.sciencedirect.com/science/article/pii/0004370292900656",
author = "Radford M. Neal",
abstract = "Connectionist learning procedures are presented for “sigmoid” and “noisy-OR” varieties of probabilistic belief networks. These networks have previously been seen primarily as a means of representing knowledge derived from experts. Here it is shown that the “Gibbs sampling” simulation procedure for such networks can support maximum-likelihood learning from empirical data through local gradient ascent. This learning procedure resembles that used for “Boltzmann machines”, and like it, allows the use of “hidden” variables to model correlations between visible variables. Due to the directed nature of the connections in a belief network, however, the “negative phase” of Boltzmann machine learning is unnecessary. Experimental results show that, as a result, learning in a sigmoid belief network can be faster than in a Boltzmann machine. These networks have other advantages over Boltzmann machines in pattern classification and decision making applications, are naturally applicable to unsupervised learning problems, and provide a link between work on connectionist learning and work on the representation of expert knowledge."
}

% https://research.google/pubs/pub40811/
% https://static.googleusercontent.com/media/research.google.com/en//pubs/archive/40811.pdf
% ReLU for speech processing
@inproceedings{40811,
title	= {On Rectified Linear Units For Speech Processing},
author	= {M.D. Zeiler and M. Ranzato and R. Monga and M. Mao and K. Yang and Q.V. Le and P. Nguyen and A. Senior and V. Vanhoucke and J. Dean and G.E. Hinton},
year	= {2013},
booktitle	= {38th International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
address	= {Vancouver}
}



% Obrazek Training vs Test set error
@article{trainingvstesterror,
	author = {Chen, Shang and Cheng, Ming-Jen},
	year = {2006},
	month = {01},
	pages = {49-58},
	title = {Building an Adaptive Evaluation System: A Design Education Application},
	volume = {3},
	journal = {Computer-Aided Design \& Applications},
	doi = {10.1080/16864360.2006.10738441}
}

% SI-SDR
% https://hal.inria.fr/inria-00544230
% https://hal.inria.fr/inria-00544230/PDF/vincent_TASLP06bis.pdf
@article{vincent-inria-00544230,
  TITLE = {{Performance measurement in blind audio source separation}},
  AUTHOR = {Vincent, Emmanuel and Gribonval, R{\'e}mi and F{\'e}votte, C{\'e}dric},
  URL = {https://hal.inria.fr/inria-00544230},
  JOURNAL = {{IEEE Transactions on Audio, Speech and Language Processing}},
  PUBLISHER = {{Institute of Electrical and Electronics Engineers}},
  VOLUME = {14},
  NUMBER = {4},
  PAGES = {1462--1469},
  YEAR = {2006},
  PDF = {https://hal.inria.fr/inria-00544230/file/vincent_TASLP06bis.pdf},
  HAL_ID = {inria-00544230},
  HAL_VERSION = {v1},
}

% https://www.researchgate.net/publication/304360600_A_weighted_STOI_intelligibility_metric_based_on_mutual_information
@inproceedings{inproceedings,
author = {Lightburn, Leo and Brookes, Mike},
year = {2016},
month = {03},
pages = {5365-5369},
title = {A weighted STOI intelligibility metric based on mutual information},
doi = {10.1109/ICASSP.2016.7472702}
}


% Obrazek PESQ 
% https://arxiv.org/abs/1901.09146
% https://arxiv.org/pdf/1901.09146.pdf
@article{sdr-pesq-optimization,
  author    = {Jaeyoung Kim and
               Mostafa El{-}Khamy and
               Jungwon Lee},
  title     = {End-to-End Multi-Task Denoising for joint {SDR} and {PESQ} Optimization},
  journal   = {CoRR},
  volume    = {abs/1901.09146},
  year      = {2019},
  url       = {http://arxiv.org/abs/1901.09146},
  archivePrefix = {arXiv},
  eprint    = {1901.09146},
  timestamp = {Sat, 02 Feb 2019 16:56:00 +0100},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1901-09146.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}


% STOI a SDR metriky
% https://arxiv.org/abs/1806.00511
% https://arxiv.org/pdf/1806.00511.pdf
@misc{venkataramani2018performance,
    title={Performance Based Cost Functions for End-to-End Speech Separation},
    author={Shrikant Venkataramani and Ryley Higa and Paris Smaragdis},
    year={2018},
    eprint={1806.00511},
    archivePrefix={arXiv},
    primaryClass={eess.AS}
}

% STOI
% https://ieeexplore.ieee.org/document/5495701
% http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.696.995
@INPROCEEDINGS{taal-stoi,
  author={C. H. {Taal} and R. C. {Hendriks} and R. {Heusdens} and J. {Jensen}},
  booktitle={2010 IEEE International Conference on Acoustics, Speech and Signal Processing}, 
  title={A short-time objective intelligibility measure for time-frequency weighted noisy speech}, 
  year={2010},
  volume={},
  number={},
  pages={4214-4217},
}

% STOI definition
% https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4452133/
% https://ieeexplore.ieee.org/abstract/document/5713237
@ARTICLE{5713237,
  author={C. H. {Taal} and R. C. {Hendriks} and R. {Heusdens} and J. {Jensen}},
  journal={IEEE Transactions on Audio, Speech, and Language Processing}, 
  title={An Algorithm for Intelligibility Prediction of Time–Frequency Weighted Noisy Speech}, 
  year={2011},
  volume={19},
  number={7},
  pages={2125-2136},
}


% Xavier algorithm for weight initialisation
% 	http://proceedings.mlr.press/v9/glorot10a.html
% 		http://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf
@InProceedings{pmlr-v9-glorot10a,
	title = 	 {Understanding the difficulty of training deep feedforward neural networks},
	author = 	 {Xavier Glorot and Yoshua Bengio},
	booktitle = 	 {Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics},
	pages = 	 {249--256},
	year = 	 {2010},
	editor = 	 {Yee Whye Teh and Mike Titterington},
	volume = 	 {9},
	series = 	 {Proceedings of Machine Learning Research},
	address = 	 {Chia Laguna Resort, Sardinia, Italy},
	month = 	 {13--15 May},
	publisher = 	 {PMLR},
	pdf = 	 {http://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf},
	url = 	 {http://proceedings.mlr.press/v9/glorot10a.html},
	abstract = 	 {Whereas before 2006 it appears that deep multi-layer neural networks were not successfully trained, since then several algorithms have been shown to successfully train them, with experimental results showing the superiority of deeper vs less deep architectures. All these experimental results were obtained with new initialization or training mechanisms. Our objective here is to understand better why standard gradient descent from random initialization is doing so poorly with deep neural networks, to better understand these recent relative successes and help design better algorithms in the future.  We first observe the influence of the non-linear activations functions. We find that the logistic sigmoid activation is unsuited for deep networks with random initialization because of its mean value, which can drive especially the top hidden layer into saturation. Surprisingly, we find that saturated units can move out of saturation by themselves, albeit slowly, and explaining the plateaus sometimes seen when training neural networks. We find that a new non-linearity that saturates less can often be beneficial. Finally, we study how activations and gradients vary across layers and during training, with the idea that training may be more difficult when the singular values of the Jacobian associated with each layer are far from 1.  Based on these considerations, we propose a new initialization scheme that brings substantially faster convergence.}
}


% [1] ze studie
@article{speechseparationoverview,
	author = {Wang, DeLiang and Chen, Jitong},
	title = {Supervised Speech Separation Based on Deep Learning: An Overview},
	year = {2018},
	issue_date = {October 2018},
	publisher = {IEEE Press},
	volume = {26},
	number = {10},
	issn = {2329-9290},
	url = {https://doi.org/10.1109/TASLP.2018.2842159},
	doi = {10.1109/TASLP.2018.2842159},
	journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
	month = oct,
	pages = {1702–1726},
	numpages = {25}
}

% [23] ze studie -- TCN
% https://arxiv.org/abs/1608.08242
@misc{lea2016temporal,
    title={Temporal Convolutional Networks: A Unified Approach to Action Segmentation},
    author={Colin Lea and Rene Vidal and Austin Reiter and Gregory D. Hager},
    year={2016},
    eprint={1608.08242},
    archivePrefix={arXiv},
    primaryClass={cs.CV}
}

% [25] ze studie -- TCN
% https://arxiv.org/abs/1803.01271
@misc{bai2018empirical,
    title={An Empirical Evaluation of Generic Convolutional and Recurrent Networks for Sequence Modeling},
    author={Shaojie Bai and J. Zico Kolter and Vladlen Koltun},
    year={2018},
    eprint={1803.01271},
    archivePrefix={arXiv},
    primaryClass={cs.LG}
}

%Batch normalization citace ze studie [47]
% https://pytorch.org/docs/master/generated/torch.nn.BatchNorm1d.html
%	https://arxiv.org/abs/1502.03167
% 		https://arxiv.org/pdf/1502.03167.pdf
@misc{ioffe2015batch,
    title={Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift},
    author={Sergey Ioffe and Christian Szegedy},
    year={2015},
    eprint={1502.03167},
    archivePrefix={arXiv},
    primaryClass={cs.LG}
}

%Layer normalization z pytorch, ale taky pouzita na Batch Norm
% https://pytorch.org/docs/master/generated/torch.nn.LayerNorm.html
% 	https://arxiv.org/abs/1607.06450
@misc{ba2016layer,
    title={Layer Normalization},
    author={Jimmy Lei Ba and Jamie Ryan Kiros and Geoffrey E. Hinton},
    year={2016},
    eprint={1607.06450},
    archivePrefix={arXiv},
    primaryClass={stat.ML}
}

% Eventuelne Web o normalizaci
%	https://mlexplained.com/2018/11/30/an-overview-of-normalization-methods-in-deep-learning/
% ------

%PReLU paper
% https://arxiv.org/abs/1502.01852
@misc{he2015delving,
    title={Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification},
    author={Kaiming He and Xiangyu Zhang and Shaoqing Ren and Jian Sun},
    year={2015},
    eprint={1502.01852},
    archivePrefix={arXiv},
    primaryClass={cs.CV}
}

%Residual connection from TasNet
% https://arxiv.org/abs/1512.03385
@misc{he2015deep,
    title={Deep Residual Learning for Image Recognition},
    author={Kaiming He and Xiangyu Zhang and Shaoqing Ren and Jian Sun},
    year={2015},
    eprint={1512.03385},
    archivePrefix={arXiv},
    primaryClass={cs.CV}
}

%Why Residual Networks Can Become Extremely Deep
% https://arxiv.org/abs/1805.07477
@misc{zaeemzadeh2018normpreservation,
    title={Norm-Preservation: Why Residual Networks Can Become Extremely Deep?},
    author={Alireza Zaeemzadeh and Nazanin Rahnavard and Mubarak Shah},
    year={2018},
    eprint={1805.07477},
    archivePrefix={arXiv},
    primaryClass={cs.CV}
}

%Backpropagation Applied to Handwritten Zip Code Recognition (CNN)
% https://ieeexplore.ieee.org/document/6795724
@ARTICLE{6795724,
  author={Y. {LeCun} and B. {Boser} and J. S. {Denker} and D. {Henderson} and R. E. {Howard} and W. {Hubbard} and L. D. {Jackel}},
  journal={Neural Computation}, 
  title={Backpropagation Applied to Handwritten Zip Code Recognition}, 
  year={1989},
  volume={1},
  number={4},
  pages={541-551},
}

%Learning long-term dependencies with gradient descent is difficult
% https://ieeexplore.ieee.org/document/279181
@ARTICLE{279181,
	author={Y. {Bengio} and P. {Simard} and P. {Frasconi}},  
	journal={IEEE Transactions on Neural Networks},
    title={Learning long-term dependencies with gradient descent is difficult},
    year={1994},
    volume={5},
    number={2},
    pages={157-166},
}


% Tlusta NN knizka z knihovny
@BOOK{mitdeeplearning,
	author =       "Ian Goodfellow, Yoshua Bengio, Aaron Courville",
	title =        "{\it DEEP LEARNING}",
	publisher =    "MIT Press",
	year =         "2017",
	isbn =         "9780262035613",
}
    
% Ta tenka knizka, ale ctivejsi o NN, CNN, atd z knihovny
@BOOK{mitdeeplearning_small,
	author =       "John D. Kelleher",
	title =        "{\it DEEP LEARNING / John D. Kelleher}",
	publisher =    "MIT Press",
	year =         "2019",
	isbn =         "9780262537551",
}
	
	
% 
@Article{speechseparation,
	author = "Qian, Y., Weng, C., Chang, X. et al.",
	year =   "2019",
	month =  "02",
	pages =  "40–63",
	title =  "Past review, current progress, and challenges ahead on the cocktail party problem",
	journal = "Frontiers Inf Technol Electronic Eng 19",
	url = "https://doi-org.ezproxy.lib.vutbr.cz/10.1631/FITEE.1700814",	
}
	

% 
@article{pesq-itut,
  author    = {Rix, A. W., Beerends, J. G., Hollier, M. P., and Hekstra,A. P.},
  title     = {erceptual evaluation of speech quality (pesq)-anew method for speech quality assessment of telephonenetworks and codecs},
  journal   = {Acoustics, Speech, and SignalProcessing},
  volume    = {2},
  year      = {2001},
  pages =  "40–63",
note="
Rix, A. W., Beerends, J. G., Hollier, M. P., and Hekstra,A. P.  Perceptual evaluation of speech quality (pesq)-anew method for speech quality assessment of telephonenetworks and codecs.  InAcoustics, Speech, and SignalProcessing, 2001. Proceedings.(ICASSP’01). 2001 IEEEInternational Conference on,  volume  2,  pp.  749–752.IEEE, 2001"
}


%	
@misc{ruder2016overview,
    title={An overview of gradient descent optimization algorithms},
    author={Sebastian Ruder},
    year={2016},
    eprint={1609.04747},
    archivePrefix={arXiv},
    primaryClass={cs.LG}
}


%
@webpage{web-backpropagation,
	author =       "John McGonagle, George Shaikouski, Christopher Williams, Andrew Hsu, Jimin Khim",
	title =        "Backpropagation, \textit{Brilliant.org}",
	howpublished = "online",
	year =         "2019",
	month =        3,
	url =          "https://brilliant.org/wiki/backpropagation/",
	cited =        "2019-04-10"
}

