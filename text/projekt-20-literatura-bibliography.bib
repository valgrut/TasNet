% ====================== Kapitola 1 =====================

% CHECKED
% Separace mluvcich a koktejl party - prepinani pozornosti
@article{GETZMANN20171,
title = "Switching of auditory attention in “cocktail-party” listening: ERP evidence of cueing effects in younger and older adults",
journal = "Brain and Cognition",
volume = "111",
pages = "1 - 12",
year = "2017",
issn = "0278-2626",
doi = "https://doi.org/10.1016/j.bandc.2016.09.006",
address = "Leibniz Research Centre for Working Environment and Human Factors, D-44139 Dortmund, Germany",
url = "http://www.sciencedirect.com/science/article/pii/S0278262616302408",
author = "Stephan Getzmann and Julian Jasny and Michael Falkenstein",
keywords = "Aging, Speech perception, Attention, Cueing, Event-related potentials",
abstract = {Verbal communication in a “cocktail-party situation” is a major challenge for the auditory system. In particular, changes in target speaker usually result in declined speech perception. Here, we investigated whether speech cues indicating a subsequent change in target speaker reduce the costs of switching in younger and older adults. We employed event-related potential (ERP) measures and a speech perception task, in which sequences of short words were simultaneously presented by four speakers. Changes in target speaker were either unpredictable or semantically cued by a word within the target stream. Cued changes resulted in a less decreased performance than uncued changes in both age groups. The ERP analysis revealed shorter latencies in the change-related N400 and late positive complex (LPC) after cued changes, suggesting an acceleration in context updating and attention switching. Thus, both younger and older listeners used semantic cues to prepare changes in speaker setting.}
}

% CHECKED
%% 26
% https://arxiv.org/abs/1610.02357
% https://arxiv.org/abs/1610.02357v3
@InProceedings{Chollet_2017_CVPR,
	author={F. {Chollet}},
	title = {Xception: Deep Learning With Depthwise Separable Convolutions},
	booktitle = {Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
	month = {July},
	year = {2017},
	volume={},
	number={},
	publisher={IEEE},
	pages={1800-1807},
	doi={10.1109/CVPR.2017.195},
	ISSN={1063-6919},
	abstract={We present an interpretation of Inception modules in convolutional neural networks as being an intermediate step in-between regular convolution and the depthwise separable convolution operation (a depthwise convolution followed by a pointwise convolution). In this light, a depthwise separable convolution can be understood as an Inception module with a maximally large number of towers. This observation leads us to propose a novel deep convolutional neural network architecture inspired by Inception, where Inception modules have been replaced with depthwise separable convolutions. We show that this architecture, dubbed Xception, slightly outperforms Inception V3 on the ImageNet dataset (which Inception V3 was designed for), and significantly outperforms Inception V3 on a larger image classification dataset comprising 350 million images and 17,000 classes. Since the Xception architecture has the same number of parameters as Inception V3, the performance gains are not due to increased capacity but rather to a more efficient use of model parameters.},
	keywords={feedforward neural nets;image classification;learning (artificial intelligence);neural net architecture;pointwise convolution;Inception module;deep convolutional neural network architecture;regular convolution;depthwise separable convolution operation;Xception;deep learning;Inception V3;ImageNet dataset;image classification dataset;Computer architecture;Correlation;Convolutional codes;Google;Biological neural networks}
}

% CHECKED
% 27
% https://dblp.uni-trier.de/rec/bibtex/journals/corr/HowardZCKWWAA17
% https://arxiv.org/abs/1704.04861
@article{MobileNets,
  author    = {Andrew G. Howard and
               Menglong Zhu and
               Bo Chen and
               Dmitry Kalenichenko and
               Weijun Wang and
               Tobias Weyand and
               Marco Andreetto and
               Hartwig Adam},
  title     = {MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications},
  journal   = {CoRR},
  volume    = {abs/1704.04861},
  year      = {2017},
  url       = {http://arxiv.org/abs/1704.04861},
  archivePrefix = {arXiv},
  eprint    = {1704.04861},
  timestamp = {Mon, 13 Aug 2018 16:46:35 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/HowardZCKWWAA17.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

% CHECKED
% Separace uvod, popis cocktail party, dosavadni metody reseni separace
% https://link-springer-com.ezproxy.lib.vutbr.cz/article/10.1631/FITEE.1700814#citeas
%
%Citace:
%Qian, Y., Weng, C., Chang, X. et al. Past review, current progress, and challenges ahead on the %cocktail party problem. Frontiers Inf Technol Electronic Eng 19, 40–63 (2018). https://doi-%org.ezproxy.lib.vutbr.cz/10.1631/FITEE.1700814
@article{cocktailparty,
  title={Past review, current progress, and challenges ahead on the cocktail party problem},
  author={Qian, Y., Weng, C., Chang, X. et al.},
  journal={Frontiers Inf Technol Electronic Eng},
  volume={19},
  number={1},
  pages={40--63},
  year={2018},
  month={Jan},
  revised={17. 1. 2018},
  issn={2095-9230},
  doi={10.1631/FITEE.1700814},
  url={https://doi.org/10.1631/FITEE.1700814},
  publisher={Springer Nature}
}

% CHECKED
% CASA ze studie
% https://pdfs.semanticscholar.org/8875/39061b4dec2439180eb58c2d6eae13f24548.pdf
@article{choi2005blind,
  title={Blind source separation and independent component analysis: A review},
  author={Choi, Seungjin and Cichocki, Andrzej and Park, Hyung-Min and Lee, Soo-Young},
  journal={Neural Information Processing-Letters and Reviews},
  volume={6},
  number={1},
  pages={1--57},
  year={2005},
  month={Jan}
}

% CHECKED
% NMF
% https://papers.nips.cc/paper/1861-algorithms-for-non-negative-matrix-factorization
% http://papers.nips.cc/paper/1861-algorithms-for-non-negative-matrix-factorization.pdf
@incollection{NIPS2000_1861,
title = {Algorithms for Non-negative Matrix Factorization},
author = {Daniel D. Lee and Seung, H. Sebastian},
booktitle = {Advances in Neural Information Processing Systems 13},
editor = {T. K. Leen and T. G. Dietterich and V. Tresp},
pages = {556--562},
year = {2001},
publisher = {MIT Press},
url = {http://papers.nips.cc/paper/1861-algorithms-for-non-negative-matrix-factorization.pdf}
}

% CHECKED
% NMF z TasNet
% http://sap.ist.i.kyoto-u.ac.jp/members/yoshii/papers/ismir-2013-yoshii.pdf
@inproceedings{yoshii2013beyond,
  title={Beyond NMF: Time-Domain Audio Source Separation without Phase Reconstruction.},
  author={Yoshii, Kazuyoshi and Tomioka, Ryota and Mochihashi, Daichi and Goto, Masataka},
  booktitle={ISMIR},
  pages={369--374},
  year={2013},
  url={http://ismir2013.ismir.net/wp-content/uploads/2013/09/32_Paper.pdf}
}

% CHECKED
% beamforming
% https://arxiv.org/abs/1212.6080
@article{adel2012beamforming,
  author    = {Adel Hidri and
               Souad Meddeb and
               Abdulqadir Alaqeeli and
               Hamid Amiri},
  title     = {Beamforming Techniques for Multichannel audio Signal Separation},
  journal   = {CoRR},
  volume    = {abs/1212.6080},
  year      = {2012},
  url       = {http://arxiv.org/abs/1212.6080},
  archivePrefix = {arXiv},
  eprint    = {1212.6080},
  timestamp = {Mon, 13 Aug 2018 16:48:57 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1212-6080.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

% CHECKED
% https://iopscience.iop.org/article/10.1088/1741-2560/13/2/026027/meta
@article{Negro_2016,
	doi = {10.1088/1741-2560/13/2/026027},
	url = {https://doi.org/10.1088%2F1741-2560%2F13%2F2%2F026027},
	year = 2016,
	month = {Feb},
	publisher = {{IOP} Publishing},
	volume = {13},
	number = {2},
	pages = {026027},
	author = {Francesco Negro and Silvia Muceli and Anna Margherita Castronovo and Ales Holobar and Dario Farina},
	title = {Multi-channel intramuscular and surface {EMG} decomposition by convolutive blind source separation},
	journal = {Journal of Neural Engineering},
	abstract = {Objective. The study of motor unit behavior has been classically performed by selective recording systems of muscle electrical activity (EMG signals) and decomposition algorithms able to discriminate between individual motor unit action potentials from multi-unit signals. In this study, we provide a general framework for the decomposition of multi-channel intramuscular and surface EMG signals and we extensively validate this approach with experimental recordings. Approach. First, we describe the conditions under which the assumptions of the convolutive blind separation model are satisfied. Second, we propose an approach of convolutive sphering of the observations followed by an iterative extraction of the sources. This approach is then validated using intramuscular signals recorded by novel multi-channel thin-film electrodes on the Abductor Digiti Minimi of the hand and Tibilias Anterior muscles, as well as on high-density surface EMG signals recorded by electrode grids on the First Dorsal Interosseous muscle. The validation was based on the comparison with the gold standard of manual decomposition (for intramuscular recordings) and on the two-source method (for comparison of intramuscular and surface EMG recordings) for the three human muscles and contraction forces of up to 90% MVC. Main results. The average number of common sources identified for the validation was 14 ± 7 (averaged across all trials and subjects and all comparisons), with a rate of agreement in their discharge timings of 92.8 ± 3.2%. The average Decomposability Index, calculated on the automatic decomposed signals, was 16.0 ± 2.2 (7.3–44.1). For comparison, the same index calculated on the manual decomposed signals was 15.0 ± 3.0 (6.3–76.6). Significance. These results show that the method provides a solid framework for the decomposition of multi-channel invasive and non-invasive EMG signals that allows the study of the behavior of a large number of concurrently active motor units.}
}

% CHECKED
% Studie, ze ktere moje prace vychazi
@article{luo2018convtasnet,
  author    = {Yi Luo and
               Nima Mesgarani},
  title     = {TasNet: Surpassing Ideal Time-Frequency Masking for Speech Separation},
  journal   = {CoRR},
  volume    = {abs/1809.07454},
  year      = {2018},
  month		= {Sep},
  url       = {http://arxiv.org/abs/1809.07454},
  archivePrefix = {arXiv},
  eprint    = {1809.07454},
  timestamp = {Fri, 05 Oct 2018 11:34:52 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1809-07454.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org},
  note 		= {v2}
}

	
% MSE, MAE, 
% https://towardsdatascience.com/understanding-the-3-most-common-loss-functions-for-machine-learning-regression-23e0ef3e14d3

% Obrazek FeedForward NN vs Recurrent NN
@article{FFandRecNN,
	author = {Pekel, Engin and Kara, Selin},
	year = {2017},
	month = {Mar},
	pages = {157-179},
	title = {A COMPREHENSIVE REVIEW FOR ARTIFICAL NEURAL NETWORK APPLICATION TO PUBLIC TRANSPORTATION},
	volume = {35},
	journal = {Sigma Journal of Engineering and Natural Sciences}
}

% Relu article citation about success of ReLU
@incollection{NIPS2012_4824,
title = {ImageNet Classification with Deep Convolutional Neural Networks},
author = {Alex Krizhevsky and Sutskever, Ilya and Hinton, Geoffrey E},
booktitle = {Advances in Neural Information Processing Systems 25},
editor = {F. Pereira and C. J. C. Burges and L. Bottou and K. Q. Weinberger},
pages = {1097--1105},
year = {2012},
publisher = {Curran Associates, Inc.},
url = {http://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf}
}

% CHECKED
% aktivacni funkce -- sigmoidy, softmax, relu a pod
% https://arxiv.org/abs/1811.03378
@article{nwankpa2018activation,
  author    = {Chigozie Nwankpa and
               Winifred Ijomah and
               Anthony Gachagan and
               Stephen Marshall},
  title     = {Activation Functions: Comparison of trends in Practice and Research for Deep Learning},
  journal   = {CoRR},
  volume    = {abs/1811.03378},
  year      = {2018},
  url       = {http://arxiv.org/abs/1811.03378},
  archivePrefix = {arXiv},
  eprint    = {1811.03378},
  timestamp = {Fri, 23 Nov 2018 12:43:51 +0100},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1811-03378.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

% CHECKED
%logistic sigmoid nevyhody a vyhody
%https://www.sciencedirect.com/science/article/abs/pii/0004370292900656?via%3Dihub
@article{NEAL199271,
title = "Connectionist learning of belief networks",
journal = "Artificial Intelligence",
volume = "56",
number = "1",
pages = "71 - 113",
year = "1992",
issn = "0004-3702",
doi = "https://doi.org/10.1016/0004-3702(92)90065-6",
url = "http://www.sciencedirect.com/science/article/pii/0004370292900656",
author = "Radford M. Neal",
abstract = {Connectionist learning procedures are presented for “sigmoid” and “noisy-OR” varieties of probabilistic belief networks. These networks have previously been seen primarily as a means of representing knowledge derived from experts. Here it is shown that the “Gibbs sampling” simulation procedure for such networks can support maximum-likelihood learning from empirical data through local gradient ascent. This learning procedure resembles that used for “Boltzmann machines”, and like it, allows the use of “hidden” variables to model correlations between visible variables. Due to the directed nature of the connections in a belief network, however, the “negative phase” of Boltzmann machine learning is unnecessary. Experimental results show that, as a result, learning in a sigmoid belief network can be faster than in a Boltzmann machine. These networks have other advantages over Boltzmann machines in pattern classification and decision making applications, are naturally applicable to unsupervised learning problems, and provide a link between work on connectionist learning and work on the representation of expert knowledge.}
}

% CHECKED
% https://research.google/pubs/pub40811/
% https://ieeexplore.ieee.org/abstract/document/6638312
% https://static.googleusercontent.com/media/research.google.com/en//pubs/archive/40811.pdf
% ReLU for speech processing
@inproceedings{40811,
	title	= {On Rectified Linear Units For Speech Processing},
    author={M. D. {Zeiler} and M. {Ranzato} and R. {Monga} and M. {Mao} and K. {Yang} and Q. V. {Le} and P. {Nguyen} and A. {Senior} and V. {Vanhoucke} and J. {Dean} and G. E. {Hinton}},
	year	= {2013},
    booktitle={2013 IEEE International Conference on Acoustics, Speech and Signal Processing}, 
	address	= {Vancouver, BC, Canada},
	pages={3517--3521},
	doi={10.1109/ICASSP.2013.6638312},
    ISSN={2379-190X},
	organization={IEEE},
	  abstract={Deep neural networks have recently become the gold standard for acoustic modeling in speech recognition systems. The key computational unit of a deep network is a linear projection followed by a point-wise non-linearity, which is typically a logistic function. In this work, we show that we can improve generalization and make training of deep networks faster and simpler by substituting the logistic units with rectified linear units. These units are linear when their input is positive and zero otherwise. In a supervised setting, we can successfully train very deep nets from random initialization on a large vocabulary speech recognition task achieving lower word error rates than using a logistic network with the same topology. Similarly in an unsupervised setting, we show how we can learn sparse features that can be useful for discriminative tasks. All our experiments are executed in a distributed environment using several hundred machines and several hundred hours of speech data.},
	keywords={neural nets;speech processing;speech recognition;vocabulary;speech processing;rectified linear units;deep neural networks;gold standard;acoustic modeling;key computational unit;linear projection;point-wise nonlinearity;logistic function;logistic units;supervised setting;large vocabulary speech recognition task;word error rates;sparse features;discriminative tasks;distributed environment;Logistics;Training;Neural networks;Accuracy;Unsupervised learning;Error analysis;Encoding;Rectified Linear units;Deep Learning;Neural Networks;Unsupervised Learning;Hybrid System}
}


% Obrazek Training vs Test set error
@article{trainingvstesterror,
	author = {Chen, Shang and Cheng, Ming-Jen},
	year = {2006},
	month = {Jan},
	pages = {49-58},
	title = {Building an Adaptive Evaluation System: A Design Education Application},
	volume = {3},
	journal = {Computer-Aided Design \& Applications},
	doi = {10.1080/16864360.2006.10738441}
}

% SI-SDR
% https://hal.inria.fr/inria-00544230
% https://hal.inria.fr/inria-00544230/PDF/vincent_TASLP06bis.pdf
@article{vincent-inria-00544230,
  TITLE = {{Performance measurement in blind audio source separation}},
  AUTHOR = {Vincent, Emmanuel and Gribonval, R{\'e}mi and F{\'e}votte, C{\'e}dric},
  URL = {https://hal.inria.fr/inria-00544230},
  JOURNAL = {{IEEE Transactions on Audio, Speech and Language Processing}},
  PUBLISHER = {{Institute of Electrical and Electronics Engineers}},
  VOLUME = {14},
  NUMBER = {4},
  PAGES = {1462--1469},
  YEAR = {2006},
  PDF = {https://hal.inria.fr/inria-00544230/file/vincent_TASLP06bis.pdf},
  HAL_ID = {inria-00544230},
  HAL_VERSION = {v1}
}

% CHECKED
% https://www.researchgate.net/publication/304360600_A_weighted_STOI_intelligibility_metric_based_on_mutual_information
% https://ieeexplore.ieee.org/document/7472702
@INPROCEEDINGS{7472702,
  author={L. {Lightburn} and M. {Brookes}},
  booktitle={2016 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)}, 
  title={A weighted STOI intelligibility metric based on mutual information}, 
  year={2016},
  volume={},
  number={},
  publisher = {IEEE},
  pages={5365-5369},
  abstract={It is known that the information required for the intelligibility of a speech signal is distributed non-uniformly in time. In this paper we propose WSTOI, a modified version of STOI, a speech intelligibility metric. With WSTOI the contribution of each time-frequency cell is weighted by an estimate of its intelligibility content. This estimate is equal to the mutual information between two hypothetical signals at either end of a simplified model of human communication. Listening tests show that the modification improves the prediction accuracy of STOI at all performance levels on both long and short utterances. An improvement was observed across all tested noise types and suppression algorithms.},
  keywords={signal denoising;speech processing;time-frequency analysis;weighted STOI intelligibility metric;mutual information;speech signal;time-frequency cell;hypothetical signals;human communication;performance levels;suppression algorithms;noise types;Speech;Measurement;Mutual information;Computational modeling;Correlation;Production;Information rates;Intelligibility;intelligibility metric;intelligibility estimate;mutual information;speech entropy},
  doi={10.1109/ICASSP.2016.7472702},
  ISSN={2379-190X},
  month={Mar}
}


% CHECKED
% Obrazek PESQ a popis pesq
% https://arxiv.org/abs/1901.09146
% https://arxiv.org/pdf/1901.09146.pdf
@article{sdr-pesq-optimization,
  author    = {Jaeyoung Kim and
               Mostafa El{-}Khamy and
               Jungwon Lee},
  title     = {End-to-End Multi-Task Denoising for joint {SDR} and {PESQ} Optimization},
  journal   = {CoRR},
  volume    = {abs/1901.09146},
  year      = {2019},
  month 	= {Jan},
  url       = {http://arxiv.org/abs/1901.09146},
  archivePrefix = {arXiv},
  eprint    = {1901.09146},
  timestamp = {Sat, 02 Feb 2019 16:56:00 +0100},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1901-09146.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

% CHECKED
% STOI a SDR metriky
% https://arxiv.org/abs/1806.00511
% https://arxiv.org/pdf/1806.00511.pdf
% https://ieeexplore.ieee.org/abstract/document/8659758
@inproceedings{venkataramani2018performance,
  title={Performance based cost functions for end-to-end speech separation},
  author=S. {Venkataramani} and R. {Higa} and P. {Smaragdis},
  booktitle={2018 Asia-Pacific Signal and Information Processing Association Annual Summit and Conference (APSIPA ASC)},
  pages={350--355},
  year={2018},
  organization={IEEE},
  abstract={Recent neural network strategies for source separation attempt to model audio signals by processing their waveforms directly. Mean squared error (MSE) that measures the Euclidean distance between waveforms of denoised speech and the ground-truth speech, has been a natural cost-function for these approaches. However, MSE is not a perceptually motivated measure and may result in large perceptual discrepancies. In this paper, we propose and experiment with new loss functions for end-to-end source separation. These loss functions are motivated by BSS_Eval and perceptual metrics like source to distortion ratio (SDR), source to interference ratio (SIR), source to artifact ratio (SAR) and short-time objective intelligibility ratio (STOI). This enables the flexibility to mix and match these loss functions depending upon the requirements of the task. Subjective listening tests reveal that combinations of the proposed cost functions help achieve superior separation performance as compared to stand-alone MSE and SDR costs.},
  keywords={audio signal processing;hearing;interference (signal);mean square error methods;neural nets;signal denoising;source separation;speech processing;end-to-end speech separation;audio signals;mean squared error;Euclidean distance;denoised speech;ground-truth speech;end-to-end source separation;short-time objective intelligibility ratio;neural network strategies;BSS_Eval;source to distortion ratio;source to interference ratio;source to artifact ratio;Source separation;Cost function;Measurement;Spectrogram;Neural networks;Interference;Convolution;End-to-end speech separation;Deep learning;Cost functions},
  doi={10.23919/APSIPA.2018.8659758},
  ISSN={2640-0103},
  month={Nov}
}

% CHECKED
% STOI
% https://ieeexplore.ieee.org/document/5495701
% http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.696.995
@INPROCEEDINGS{taal-stoi,
  author={C. H. {Taal} and R. C. {Hendriks} and R. {Heusdens} and J. {Jensen}},
  booktitle={2010 IEEE International Conference on Acoustics, Speech and Signal Processing}, 
  title={A short-time objective intelligibility measure for time-frequency weighted noisy speech}, 
  year={2010},
  volume={},
  number={},
  pages={4214-4217},
  abstract={Existing objective speech-intelligibility measures are suitable for several types of degradation, however, it turns out that they are less appropriate for methods where noisy speech is processed by a time-frequency (TF) weighting, e.g., noise reduction and speech separation. In this paper, we present an objective intelligibility measure, which shows high correlation (rho=0.95) with the intelligibility of both noisy, and TF-weighted noisy speech. The proposed method shows significantly better performance than three other, more sophisticated, objective measures. Furthermore, it is based on an intermediate intelligibility measure for short-time (approximately 400 ms) TF-regions, and uses a simple DFT-based TF-decomposition. In addition, a free Matlab implementation is provided.},
  keywords={speech enhancement;speech intelligibility;time-frequency weighted noisy speech;objective speech-intelligibility measurement;time-frequency weighting;noise reduction;speech separation;objective intelligibility measurement;Weight measurement;Time frequency analysis;Speech processing;Speech enhancement;Degradation;Artificial intelligence;Noise reduction;Signal processing;Noise measurement;Testing;intelligibility prediction;speech enhancement;noisy speech},
  doi={10.1109/ICASSP.2010.5495701},
  ISSN={2379-190X},
  month={Mar}
}

% CHECKED
% STOI definition
% https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4452133/
% https://ieeexplore.ieee.org/abstract/document/5713237
@ARTICLE{5713237,
  author={C. H. {Taal} and R. C. {Hendriks} and R. {Heusdens} and J. {Jensen}},
  journal={IEEE Transactions on Audio, Speech, and Language Processing}, 
  title={An Algorithm for Intelligibility Prediction of Time–Frequency Weighted Noisy Speech}, 
  year={2011},
  volume={19},
  number={7},
  pages={2125-2136},
  abstract={In the development process of noise-reduction algorithms, an objective machine-driven intelligibility measure which shows high correlation with speech intelligibility is of great interest. Besides reducing time and costs compared to real listening experiments, an objective intelligibility measure could also help provide answers on how to improve the intelligibility of noisy unprocessed speech. In this paper, a short-time objective intelligibility measure (STOI) is presented, which shows high correlation with the intelligibility of noisy and time-frequency weighted noisy speech (e.g., resulting from noise reduction) of three different listening experiments. In general, STOI showed better correlation with speech intelligibility compared to five other reference objective intelligibility models. In contrast to other conventional intelligibility models which tend to rely on global statistics across entire sentences, STOI is based on shorter time segments (386 ms). Experiments indeed show that it is beneficial to take segment lengths of this order into account. In addition, a free Matlab implementation is provided.},
  keywords={speech enhancement;speech intelligibility;intelligibility prediction;time-frequency weighted noisy speech;noise-reduction algorithm;objective machine-driven intelligibility measure;speech intelligibility;noisy unprocessed speech;short-time objective intelligibility measure;objective intelligibility model;Speech;Noise measurement;Correlation;Speech processing;Signal to noise ratio;Time frequency analysis;Noise reduction;objective measure;speech enhancement;speech intelligibility prediction},
  doi={10.1109/TASL.2011.2114881},
  ISSN={1558-7924},
  month={Sep}
}

% CHECKED
% Xavier algorithm for weight initialisation
% 	http://proceedings.mlr.press/v9/glorot10a.html
% 		http://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf
@InProceedings{pmlr-v9-glorot10a,
	title = 	 {Understanding the difficulty of training deep feedforward neural networks},
	author = 	 {Xavier Glorot and Yoshua Bengio},
	booktitle = 	 {Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics},
	pages = 	 {249--256},
	year = 	 	 {2010},
	editor = 	 {Yee Whye Teh and Mike Titterington},
	volume = 	 {9},
	series = 	 {Proceedings of Machine Learning Research},
	address = 	 {Chia Laguna Resort, Sardinia, Italy},
	month = 	 {13--15 May},
	publisher = 	 {PMLR},
	pdf = 	 {http://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf},
	url = 	 {http://proceedings.mlr.press/v9/glorot10a.html},
	abstract = 	 {Whereas before 2006 it appears that deep multi-layer neural networks were not successfully trained, since then several algorithms have been shown to successfully train them, with experimental results showing the superiority of deeper vs less deep architectures. All these experimental results were obtained with new initialization or training mechanisms. Our objective here is to understand better why standard gradient descent from random initialization is doing so poorly with deep neural networks, to better understand these recent relative successes and help design better algorithms in the future.  We first observe the influence of the non-linear activations functions. We find that the logistic sigmoid activation is unsuited for deep networks with random initialization because of its mean value, which can drive especially the top hidden layer into saturation. Surprisingly, we find that saturated units can move out of saturation by themselves, albeit slowly, and explaining the plateaus sometimes seen when training neural networks. We find that a new non-linearity that saturates less can often be beneficial. Finally, we study how activations and gradients vary across layers and during training, with the idea that training may be more difficult when the singular values of the Jacobian associated with each layer are far from 1.  Based on these considerations, we propose a new initialization scheme that brings substantially faster convergence.}
}


% CHECKED
% [1] ze studie
@article{speechseparationoverview,
	author = {Wang, DeLiang and Chen, Jitong},
	title = {Supervised Speech Separation Based on Deep Learning: An Overview},
	year = {2018},
	issue_date = {October 2018},
	publisher = {IEEE Press},
	volume = {26},
	number = {10},
	issn = {2329-9290},
	url = {https://doi.org/10.1109/TASLP.2018.2842159},
	doi = {10.1109/TASLP.2018.2842159},
	journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
	month = oct,
	pages = {1702–1726},
	numpages = {25}
}

% CHECKED
% [23] ze studie -- TCN
% https://arxiv.org/abs/1608.08242
% https://link.springer.com/chapter/10.1007/978-3-319-49409-8_7
%@InProceedings{10.1007/978-3-319-49409-8_7,
@InProceedings{lea2016temporal,
author={Lea, Colin and Vidal, Ren{\'e} and Reiter, Austin and Hager, Gregory D.},
editor={Hua, Gang and J{\'e}gou, Herv{\'e}},
title="Temporal Convolutional Networks: A Unified Approach to Action Segmentation",
booktitle="Computer Vision -- ECCV 2016 Workshops",
year="2016",
publisher="Springer International Publishing",
address="Cham",
pages="47--54",
abstract={The dominant paradigm for video-based action segmentation is composed of two steps: first, compute low-level features for each frame using Dense Trajectories or a Convolutional Neural Network to encode local spatiotemporal information, and second, input these features into a classifier such as a Recurrent Neural Network (RNN) that captures high-level temporal relationships. While often effective, this decoupling requires specifying two separate models, each with their own complexities, and prevents capturing more nuanced long-range spatiotemporal relationships. We propose a unified approach, as demonstrated by our Temporal Convolutional Network (TCN), that hierarchically captures relationships at low-, intermediate-, and high-level time-scales. Our model achieves superior or competitive performance using video or sensor data on three public action segmentation datasets and can be trained in a fraction of the time it takes to train an RNN.},
isbn="978-3-319-49409-8"
}


% [25] ze studie -- TCN
% https://arxiv.org/abs/1803.01271
@misc{bai2018empirical,
    title={An Empirical Evaluation of Generic Convolutional and Recurrent Networks for Sequence Modeling},
    author={Shaojie Bai and J. Zico Kolter and Vladlen Koltun},
    year={2018},
    eprint={1803.01271},
    archivePrefix={arXiv},
    primaryClass={cs.LG}
}

%Batch normalization citace ze studie [47]
% https://pytorch.org/docs/master/generated/torch.nn.BatchNorm1d.html
%	https://arxiv.org/abs/1502.03167
% 		https://arxiv.org/pdf/1502.03167.pdf
@misc{ioffe2015batch,
    title={Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift},
    author={Sergey Ioffe and Christian Szegedy},
    year={2015},
    eprint={1502.03167},
    archivePrefix={arXiv},
    primaryClass={cs.LG}
}

%Layer normalization z pytorch, ale taky pouzita na Batch Norm
% https://pytorch.org/docs/master/generated/torch.nn.LayerNorm.html
% 	https://arxiv.org/abs/1607.06450
@misc{ba2016layer,
    title={Layer Normalization},
    author={Jimmy Lei Ba and Jamie Ryan Kiros and Geoffrey E. Hinton},
    year={2016},
    eprint={1607.06450},
    archivePrefix={arXiv},
    primaryClass={stat.ML}
}

% Eventuelne Web o normalizaci
%	https://mlexplained.com/2018/11/30/an-overview-of-normalization-methods-in-deep-learning/
% ------

%PReLU paper
% https://arxiv.org/abs/1502.01852
@misc{he2015delving,
    title={Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification},
    author={Kaiming He and Xiangyu Zhang and Shaoqing Ren and Jian Sun},
    year={2015},
    eprint={1502.01852},
    archivePrefix={arXiv},
    primaryClass={cs.CV}
}

%Residual connection from TasNet
% https://arxiv.org/abs/1512.03385
@misc{he2015deep,
    title={Deep Residual Learning for Image Recognition},
    author={Kaiming He and Xiangyu Zhang and Shaoqing Ren and Jian Sun},
    year={2015},
    eprint={1512.03385},
    archivePrefix={arXiv},
    primaryClass={cs.CV}
}

%Why Residual Networks Can Become Extremely Deep
% https://arxiv.org/abs/1805.07477
@misc{zaeemzadeh2018normpreservation,
    title={Norm-Preservation: Why Residual Networks Can Become Extremely Deep?},
    author={Alireza Zaeemzadeh and Nazanin Rahnavard and Mubarak Shah},
    year={2018},
    eprint={1805.07477},
    archivePrefix={arXiv},
    primaryClass={cs.CV}
}

%Backpropagation Applied to Handwritten Zip Code Recognition (CNN)
% https://ieeexplore.ieee.org/document/6795724
@ARTICLE{6795724,
  author={Y. {LeCun} and B. {Boser} and J. S. {Denker} and D. {Henderson} and R. E. {Howard} and W. {Hubbard} and L. D. {Jackel}},
  journal={Neural Computation}, 
  title={Backpropagation Applied to Handwritten Zip Code Recognition}, 
  year={1989},
  volume={1},
  number={4},
  pages={541-551},
}

%Learning long-term dependencies with gradient descent is difficult
% https://ieeexplore.ieee.org/document/279181
@ARTICLE{279181,
	author={Y. {Bengio} and P. {Simard} and P. {Frasconi}},  
	journal={IEEE Transactions on Neural Networks},
    title={Learning long-term dependencies with gradient descent is difficult},
    year={1994},
    volume={5},
    number={2},
    pages={157-166},
}


% Tlusta NN knizka z knihovny
@BOOK{mitdeeplearning,
	author =       "Ian Goodfellow, Yoshua Bengio, Aaron Courville",
	title =        "{\it DEEP LEARNING}",
	publisher =    "MIT Press",
	year =         "2017",
	isbn =         "9780262035613",
}
    
% Ta tenka knizka, ale ctivejsi o NN, CNN, atd z knihovny
@BOOK{mitdeeplearning_small,
	author =       "John D. Kelleher",
	title =        "{\it DEEP LEARNING / John D. Kelleher}",
	publisher =    "MIT Press",
	year =         "2019",
	isbn =         "9780262537551",
}
	
	
% 
@Article{speechseparation,
	author = "Qian, Y., Weng, C., Chang, X. et al.",
	year =   "2019",
	month =  "02",
	pages =  "40–63",
	title =  "Past review, current progress, and challenges ahead on the cocktail party problem",
	journal = "Frontiers Inf Technol Electronic Eng 19",
	url = "https://doi-org.ezproxy.lib.vutbr.cz/10.1631/FITEE.1700814",	
}
	

% 
@article{pesq-itut,
  author    = {Rix, A. W., Beerends, J. G., Hollier, M. P., and Hekstra,A. P.},
  title     = {erceptual evaluation of speech quality (pesq)-anew method for speech quality assessment of telephonenetworks and codecs},
  journal   = {Acoustics, Speech, and SignalProcessing},
  volume    = {2},
  year      = {2001},
  pages =  "40–63",
note="
Rix, A. W., Beerends, J. G., Hollier, M. P., and Hekstra,A. P.  Perceptual evaluation of speech quality (pesq)-anew method for speech quality assessment of telephonenetworks and codecs.  InAcoustics, Speech, and SignalProcessing, 2001. Proceedings.(ICASSP’01). 2001 IEEEInternational Conference on,  volume  2,  pp.  749–752.IEEE, 2001"
}

% ze studie odkaz na trenovaci dataset Journal
% Deep clustering: Discriminative embeddings for segmentation and separation
% https://arxiv.org/abs/1508.04306
% https://arxiv.org/pdf/1508.04306.pdf
@article{DBLP-dataset,
  author    = {John R. Hershey and
               Zhuo Chen and
               Jonathan Le Roux and
               Shinji Watanabe},
  title     = {Deep clustering: Discriminative embeddings for segmentation and separation},
  journal   = {CoRR},
  volume    = {abs/1508.04306},
  year      = {2015},
  url       = {http://arxiv.org/abs/1508.04306},
  archivePrefix = {arXiv},
  eprint    = {1508.04306},
  timestamp = {Mon, 29 Jun 2020 12:03:43 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/HersheyCRW15.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}



%	
@misc{ruder2016overview,
    title={An overview of gradient descent optimization algorithms},
    author={Sebastian Ruder},
    year={2016},
    eprint={1609.04747},
    archivePrefix={arXiv},
    primaryClass={cs.LG}
}


%
@webpage{web-backpropagation,
	author =       "John McGonagle, George Shaikouski, Christopher Williams, Andrew Hsu, Jimin Khim",
	title =        "Backpropagation, \textit{Brilliant.org}",
	howpublished = "online",
	year =         "2019",
	month =        3,
	url =          "https://brilliant.org/wiki/backpropagation/",
	cited =        "2019-04-10"
}

