@misc{luo2018convtasnet,
    title={Conv-TasNet: Surpassing Ideal Time-Frequency Magnitude Masking for Speech Separation},
    author={Yi Luo and Nima Mesgarani},
    year={2018},
    eprint={1809.07454},
    archivePrefix={arXiv},
    primaryClass={cs.SD}
}

% MSE, MAE, 
% https://towardsdatascience.com/understanding-the-3-most-common-loss-functions-for-machine-learning-regression-23e0ef3e14d3

% Obrazek FeedForward NN vs Recurrent NN
@article{FFandRecNN,
	author = {Pekel, Engin and Kara, Selin},
	year = {2017},
	month = {03},
	pages = {157-179},
	title = {A COMPREHENSIVE REVIEW FOR ARTIFICAL NEURAL NETWORK APPLICATION TO PUBLIC TRANSPORTATION},
	volume = {35},
	journal = {Sigma Journal of Engineering and Natural Sciences}
}

% Relu article citation about success of ReLU
@incollection{NIPS2012_4824,
title = {ImageNet Classification with Deep Convolutional Neural Networks},
author = {Alex Krizhevsky and Sutskever, Ilya and Hinton, Geoffrey E},
booktitle = {Advances in Neural Information Processing Systems 25},
editor = {F. Pereira and C. J. C. Burges and L. Bottou and K. Q. Weinberger},
pages = {1097--1105},
year = {2012},
publisher = {Curran Associates, Inc.},
url = {http://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf}
}

% aktivacni funkce -- sigmoidy, softmax, relu a pod
% https://arxiv.org/abs/1811.03378
@misc{nwankpa2018activation,
    title={Activation Functions: Comparison of trends in Practice and Research for Deep Learning},
    author={Chigozie Nwankpa and Winifred Ijomah and Anthony Gachagan and Stephen Marshall},
    year={2018},
    eprint={1811.03378},
    archivePrefix={arXiv},
    primaryClass={cs.LG}
}

%logistic sigmoid nevyhody a vyhody
%https://www.sciencedirect.com/science/article/abs/pii/0004370292900656?via%3Dihub
@article{NEAL199271,
title = "Connectionist learning of belief networks",
journal = "Artificial Intelligence",
volume = "56",
number = "1",
pages = "71 - 113",
year = "1992",
issn = "0004-3702",
doi = "https://doi.org/10.1016/0004-3702(92)90065-6",
url = "http://www.sciencedirect.com/science/article/pii/0004370292900656",
author = "Radford M. Neal",
abstract = "Connectionist learning procedures are presented for “sigmoid” and “noisy-OR” varieties of probabilistic belief networks. These networks have previously been seen primarily as a means of representing knowledge derived from experts. Here it is shown that the “Gibbs sampling” simulation procedure for such networks can support maximum-likelihood learning from empirical data through local gradient ascent. This learning procedure resembles that used for “Boltzmann machines”, and like it, allows the use of “hidden” variables to model correlations between visible variables. Due to the directed nature of the connections in a belief network, however, the “negative phase” of Boltzmann machine learning is unnecessary. Experimental results show that, as a result, learning in a sigmoid belief network can be faster than in a Boltzmann machine. These networks have other advantages over Boltzmann machines in pattern classification and decision making applications, are naturally applicable to unsupervised learning problems, and provide a link between work on connectionist learning and work on the representation of expert knowledge."
}

% https://research.google/pubs/pub40811/
% https://static.googleusercontent.com/media/research.google.com/en//pubs/archive/40811.pdf
% ReLU for speech processing
@inproceedings{40811,
title	= {On Rectified Linear Units For Speech Processing},
author	= {M.D. Zeiler and M. Ranzato and R. Monga and M. Mao and K. Yang and Q.V. Le and P. Nguyen and A. Senior and V. Vanhoucke and J. Dean and G.E. Hinton},
year	= {2013},
booktitle	= {38th International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
address	= {Vancouver}
}



% Obrazek Training vs Test set error
@article{trainingvstesterror,
	author = {Chen, Shang and Cheng, Ming-Jen},
	year = {2006},
	month = {01},
	pages = {49-58},
	title = {Building an Adaptive Evaluation System: A Design Education Application},
	volume = {3},
	journal = {Computer-Aided Design \& Applications},
	doi = {10.1080/16864360.2006.10738441}
}


% Xavier algorithm for weight initialisation
% 	http://proceedings.mlr.press/v9/glorot10a.html
% 		http://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf
@InProceedings{pmlr-v9-glorot10a,
	title = 	 {Understanding the difficulty of training deep feedforward neural networks},
	author = 	 {Xavier Glorot and Yoshua Bengio},
	booktitle = 	 {Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics},
	pages = 	 {249--256},
	year = 	 {2010},
	editor = 	 {Yee Whye Teh and Mike Titterington},
	volume = 	 {9},
	series = 	 {Proceedings of Machine Learning Research},
	address = 	 {Chia Laguna Resort, Sardinia, Italy},
	month = 	 {13--15 May},
	publisher = 	 {PMLR},
	pdf = 	 {http://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf},
	url = 	 {http://proceedings.mlr.press/v9/glorot10a.html},
	abstract = 	 {Whereas before 2006 it appears that deep multi-layer neural networks were not successfully trained, since then several algorithms have been shown to successfully train them, with experimental results showing the superiority of deeper vs less deep architectures. All these experimental results were obtained with new initialization or training mechanisms. Our objective here is to understand better why standard gradient descent from random initialization is doing so poorly with deep neural networks, to better understand these recent relative successes and help design better algorithms in the future.  We first observe the influence of the non-linear activations functions. We find that the logistic sigmoid activation is unsuited for deep networks with random initialization because of its mean value, which can drive especially the top hidden layer into saturation. Surprisingly, we find that saturated units can move out of saturation by themselves, albeit slowly, and explaining the plateaus sometimes seen when training neural networks. We find that a new non-linearity that saturates less can often be beneficial. Finally, we study how activations and gradients vary across layers and during training, with the idea that training may be more difficult when the singular values of the Jacobian associated with each layer are far from 1.  Based on these considerations, we propose a new initialization scheme that brings substantially faster convergence.}
}


% [1] ze studie
@article{speechseparationoverview,
	author = {Wang, DeLiang and Chen, Jitong},
	title = {Supervised Speech Separation Based on Deep Learning: An Overview},
	year = {2018},
	issue_date = {October 2018},
	publisher = {IEEE Press},
	volume = {26},
	number = {10},
	issn = {2329-9290},
	url = {https://doi.org/10.1109/TASLP.2018.2842159},
	doi = {10.1109/TASLP.2018.2842159},
	journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
	month = oct,
	pages = {1702–1726},
	numpages = {25}
}

%Batch normalization citace ze studie [47]
% https://pytorch.org/docs/master/generated/torch.nn.BatchNorm1d.html
%	https://arxiv.org/abs/1502.03167
% 		https://arxiv.org/pdf/1502.03167.pdf
@misc{ioffe2015batch,
    title={Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift},
    author={Sergey Ioffe and Christian Szegedy},
    year={2015},
    eprint={1502.03167},
    archivePrefix={arXiv},
    primaryClass={cs.LG}
}

%Layer normalization z pytorch, ale taky pouzita na Batch Norm
% https://pytorch.org/docs/master/generated/torch.nn.LayerNorm.html
% 	https://arxiv.org/abs/1607.06450
@misc{ba2016layer,
    title={Layer Normalization},
    author={Jimmy Lei Ba and Jamie Ryan Kiros and Geoffrey E. Hinton},
    year={2016},
    eprint={1607.06450},
    archivePrefix={arXiv},
    primaryClass={stat.ML}
}

% Eventuelne Web o normalizaci
%	https://mlexplained.com/2018/11/30/an-overview-of-normalization-methods-in-deep-learning/
% ------

%PReLU paper
% https://arxiv.org/abs/1502.01852
@misc{he2015delving,
    title={Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification},
    author={Kaiming He and Xiangyu Zhang and Shaoqing Ren and Jian Sun},
    year={2015},
    eprint={1502.01852},
    archivePrefix={arXiv},
    primaryClass={cs.CV}
}

%Residual connection from TasNet
% https://arxiv.org/abs/1512.03385
@misc{he2015deep,
    title={Deep Residual Learning for Image Recognition},
    author={Kaiming He and Xiangyu Zhang and Shaoqing Ren and Jian Sun},
    year={2015},
    eprint={1512.03385},
    archivePrefix={arXiv},
    primaryClass={cs.CV}
}

%Why Residual Networks Can Become Extremely Deep
% https://arxiv.org/abs/1805.07477
@misc{zaeemzadeh2018normpreservation,
    title={Norm-Preservation: Why Residual Networks Can Become Extremely Deep?},
    author={Alireza Zaeemzadeh and Nazanin Rahnavard and Mubarak Shah},
    year={2018},
    eprint={1805.07477},
    archivePrefix={arXiv},
    primaryClass={cs.CV}
}

%Backpropagation Applied to Handwritten Zip Code Recognition (CNN)
% https://ieeexplore.ieee.org/document/6795724
@ARTICLE{6795724,
  author={Y. {LeCun} and B. {Boser} and J. S. {Denker} and D. {Henderson} and R. E. {Howard} and W. {Hubbard} and L. D. {Jackel}},
  journal={Neural Computation}, 
  title={Backpropagation Applied to Handwritten Zip Code Recognition}, 
  year={1989},
  volume={1},
  number={4},
  pages={541-551},
}

%Learning long-term dependencies with gradient descent is difficult
% https://ieeexplore.ieee.org/document/279181
@ARTICLE{279181,
	author={Y. {Bengio} and P. {Simard} and P. {Frasconi}},  
	journal={IEEE Transactions on Neural Networks},
    title={Learning long-term dependencies with gradient descent is difficult},
    year={1994},
    volume={5},
    number={2},
    pages={157-166},
}


% Tlusta NN knizka z knihovny
@BOOK{mitdeeplearning,
	author =       "Ian Goodfellow, Yoshua Bengio, Aaron Courville",
	title =        "{\it DEEP LEARNING}",
	publisher =    "MIT Press",
	year =         "2017",
	isbn =         "9780262035613",
}
    
% Ta tenka knizka, ale ctivejsi o NN, CNN, atd z knihovny
@BOOK{mitdeeplearning_small,
	author =       "John D. Kelleher",
	title =        "{\it DEEP LEARNING / John D. Kelleher}",
	publisher =    "MIT Press",
	year =         "2019",
	isbn =         "9780262537551",
}
	
	
% 
@Article{speechseparation,
	author = "Qian, Y., Weng, C., Chang, X. et al.",
	year =   "2019",
	month =  "02",
	pages =  "40–63",
	title =  "Past review, current progress, and challenges ahead on the cocktail party problem",
	journal = "Frontiers Inf Technol Electronic Eng 19",
	url = "https://doi-org.ezproxy.lib.vutbr.cz/10.1631/FITEE.1700814",	
}
	
	
%  
@article{DBLP:journals/corr/abs-1901-09146,
  author    = {Jaeyoung Kim and
               Mostafa El{-}Khamy and
               Jungwon Lee},
  title     = {End-to-End Multi-Task Denoising for joint {SDR} and {PESQ} Optimization},
  journal   = {CoRR},
  volume    = {abs/1901.09146},
  year      = {2019},
  url       = {http://arxiv.org/abs/1901.09146},
  archivePrefix = {arXiv},
  eprint    = {1901.09146},
  timestamp = {Sat, 02 Feb 2019 16:56:00 +0100},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1901-09146.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

% 
@article{pesq-itut,
  author    = {Rix, A. W., Beerends, J. G., Hollier, M. P., and Hekstra,A. P.},
  title     = {erceptual evaluation of speech quality (pesq)-anew method for speech quality assessment of telephonenetworks and codecs},
  journal   = {Acoustics, Speech, and SignalProcessing},
  volume    = {2},
  year      = {2001},
  pages =  "40–63",
note="
Rix, A. W., Beerends, J. G., Hollier, M. P., and Hekstra,A. P.  Perceptual evaluation of speech quality (pesq)-anew method for speech quality assessment of telephonenetworks and codecs.  InAcoustics, Speech, and SignalProcessing, 2001. Proceedings.(ICASSP’01). 2001 IEEEInternational Conference on,  volume  2,  pp.  749–752.IEEE, 2001"
}


%	
@misc{ruder2016overview,
    title={An overview of gradient descent optimization algorithms},
    author={Sebastian Ruder},
    year={2016},
    eprint={1609.04747},
    archivePrefix={arXiv},
    primaryClass={cs.LG}
}


%
@webpage{web-backpropagation,
	author =       "John McGonagle, George Shaikouski, Christopher Williams, Andrew Hsu, Jimin Khim",
	title =        "Backpropagation, \textit{Brilliant.org}",
	howpublished = "online",
	year =         "2019",
	month =        3,
	url =          "https://brilliant.org/wiki/backpropagation/",
	cited =        "2019-04-10"
}

