% ====================== Kapitola 1 =====================

% CHECKED
% Separace mluvcich a koktejl party - prepinani pozornosti
@article{GETZMANN20171,
title = "Switching of auditory attention in “cocktail-party” listening: ERP evidence of cueing effects in younger and older adults",
journal = "Brain and Cognition",
volume = "111",
pages = "1 - 12",
year = "2017",
issn = "0278-2626",
doi = "https://doi.org/10.1016/j.bandc.2016.09.006",
address = "Leibniz Research Centre for Working Environment and Human Factors, D-44139 Dortmund, Germany",
url = "http://www.sciencedirect.com/science/article/pii/S0278262616302408",
author = "Stephan Getzmann and Julian Jasny and Michael Falkenstein",
keywords = "Aging, Speech perception, Attention, Cueing, Event-related potentials",
abstract = {Verbal communication in a “cocktail-party situation” is a major challenge for the auditory system. In particular, changes in target speaker usually result in declined speech perception. Here, we investigated whether speech cues indicating a subsequent change in target speaker reduce the costs of switching in younger and older adults. We employed event-related potential (ERP) measures and a speech perception task, in which sequences of short words were simultaneously presented by four speakers. Changes in target speaker were either unpredictable or semantically cued by a word within the target stream. Cued changes resulted in a less decreased performance than uncued changes in both age groups. The ERP analysis revealed shorter latencies in the change-related N400 and late positive complex (LPC) after cued changes, suggesting an acceleration in context updating and attention switching. Thus, both younger and older listeners used semantic cues to prepare changes in speaker setting.}
}

% CHECKED
%% 26
% https://arxiv.org/abs/1610.02357
% https://arxiv.org/abs/1610.02357v3
@InProceedings{Chollet_2017_CVPR,
	author={F. {Chollet}},
	title = {Xception: Deep Learning With Depthwise Separable Convolutions},
	booktitle = {Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
	month = {July},
	year = {2017},
	volume={},
	number={},
	publisher={IEEE},
	pages={1800-1807},
	doi={10.1109/CVPR.2017.195},
	ISSN={1063-6919},
	abstract={We present an interpretation of Inception modules in convolutional neural networks as being an intermediate step in-between regular convolution and the depthwise separable convolution operation (a depthwise convolution followed by a pointwise convolution). In this light, a depthwise separable convolution can be understood as an Inception module with a maximally large number of towers. This observation leads us to propose a novel deep convolutional neural network architecture inspired by Inception, where Inception modules have been replaced with depthwise separable convolutions. We show that this architecture, dubbed Xception, slightly outperforms Inception V3 on the ImageNet dataset (which Inception V3 was designed for), and significantly outperforms Inception V3 on a larger image classification dataset comprising 350 million images and 17,000 classes. Since the Xception architecture has the same number of parameters as Inception V3, the performance gains are not due to increased capacity but rather to a more efficient use of model parameters.},
	keywords={feedforward neural nets;image classification;learning (artificial intelligence);neural net architecture;pointwise convolution;Inception module;deep convolutional neural network architecture;regular convolution;depthwise separable convolution operation;Xception;deep learning;Inception V3;ImageNet dataset;image classification dataset;Computer architecture;Correlation;Convolutional codes;Google;Biological neural networks}
}

% CHECKED
% 27
% https://dblp.uni-trier.de/rec/bibtex/journals/corr/HowardZCKWWAA17
% https://arxiv.org/abs/1704.04861
@article{MobileNets,
  author    = {Andrew G. Howard and
               Menglong Zhu and
               Bo Chen and
               Dmitry Kalenichenko and
               Weijun Wang and
               Tobias Weyand and
               Marco Andreetto and
               Hartwig Adam},
  title     = {MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications},
  journal   = {CoRR},
  volume    = {abs/1704.04861},
  year      = {2017},
  url       = {http://arxiv.org/abs/1704.04861},
  archivePrefix = {arXiv},
  eprint    = {1704.04861},
  timestamp = {Mon, 13 Aug 2018 16:46:35 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/HowardZCKWWAA17.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

% CHECKED
% Separace uvod, popis cocktail party, dosavadni metody reseni separace
% https://link-springer-com.ezproxy.lib.vutbr.cz/article/10.1631/FITEE.1700814#citeas
%
%Citace:
%Qian, Y., Weng, C., Chang, X. et al. Past review, current progress, and challenges ahead on the %cocktail party problem. Frontiers Inf Technol Electronic Eng 19, 40–63 (2018). https://doi-%org.ezproxy.lib.vutbr.cz/10.1631/FITEE.1700814
%https://doi-org.ezproxy.lib.vutbr.cz/10.1631/FITEE.1700814
@article{speechseparation,
  title={Past review, current progress, and challenges ahead on the cocktail party problem},
  author={Qian, Y., Weng, C., Chang, X. et al.},
  journal={Frontiers Inf Technol Electronic Eng 19},
  volume={19},
  number={1},
  pages={40--63},
  year={2018},
  month={Jan},
  revised={17 Jan 2018},
  issn={2095-9230},
  doi={10.1631/FITEE.1700814},
  url={https://doi.org/10.1631/FITEE.1700814},
  publisher={Springer Nature}
}

% Obrazek spektrogramu reci
% https://bastibe.de/2019-09-20-analyzing-speech-signals-in-time-and-frequency.html
@webpage{speech-spectrogram,
  author={Bastian Bechtold},
  title={Basti's Scratchpad on the Internet},
  secondarytitle={Analyzing Speech Signals in Time and Frequency},
  year={2019},
  month={Sep},
  day={20},
  revised={},
  issn={},
  url={https://bastibe.de/},
  path={Projects; Other posts; Analyzing Speech Signals in Time and Frequency},
  cited={2020-07-25},
  howpublished={online},
  note={\url{https://bastibe.de/2019-09-20-analyzing-speech-signals-in-time-and-frequency.html}}
}

% CHECKED
% CASA ze studie
% https://pdfs.semanticscholar.org/8875/39061b4dec2439180eb58c2d6eae13f24548.pdf
@article{choi2005blind,
  title={Blind source separation and independent component analysis: A review},
  author={Choi, Seungjin and Cichocki, Andrzej and Park, Hyung-Min and Lee, Soo-Young},
  journal={Neural Information Processing-Letters and Reviews},
  volume={6},
  number={1},
  pages={1--57},
  year={2005},
  month={Jan}
}

% CHECKED
% NMF
% https://papers.nips.cc/paper/1861-algorithms-for-non-negative-matrix-factorization
% http://papers.nips.cc/paper/1861-algorithms-for-non-negative-matrix-factorization.pdf
@incollection{NIPS2000_1861,
title = {Algorithms for Non-negative Matrix Factorization},
author = {Daniel D. Lee and Seung, H. Sebastian},
booktitle = {Advances in Neural Information Processing Systems 13},
editor = {T. K. Leen and T. G. Dietterich and V. Tresp},
pages = {556--562},
year = {2001},
publisher = {MIT Press},
url = {http://papers.nips.cc/paper/1861-algorithms-for-non-negative-matrix-factorization.pdf}
}

% CHECKED
% NMF z TasNet
% http://sap.ist.i.kyoto-u.ac.jp/members/yoshii/papers/ismir-2013-yoshii.pdf
@inproceedings{yoshii2013beyond,
  title={Beyond NMF: Time-Domain Audio Source Separation without Phase Reconstruction.},
  author={Yoshii, Kazuyoshi and Tomioka, Ryota and Mochihashi, Daichi and Goto, Masataka},
  booktitle={ISMIR},
  pages={369--374},
  year={2013},
  url={http://ismir2013.ismir.net/wp-content/uploads/2013/09/32_Paper.pdf}
}

% CHECKED
% beamforming
% https://arxiv.org/abs/1212.6080
@article{adel2012beamforming,
  author    = {Adel Hidri and
               Souad Meddeb and
               Abdulqadir Alaqeeli and
               Hamid Amiri},
  title     = {Beamforming Techniques for Multichannel audio Signal Separation},
  journal   = {CoRR},
  volume    = {abs/1212.6080},
  year      = {2012},
  url       = {http://arxiv.org/abs/1212.6080},
  archivePrefix = {arXiv},
  eprint    = {1212.6080},
  timestamp = {Mon, 13 Aug 2018 16:48:57 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1212-6080.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

% CHECKED
% https://iopscience.iop.org/article/10.1088/1741-2560/13/2/026027/meta
@article{Negro_2016,
	doi = {10.1088/1741-2560/13/2/026027},
	url = {http://dx.doi.org/10.1088/1741-2560/13/2/026027},
	year = 2016,
	month = {Feb},
	publisher = {{IOP} Publishing},
	volume = {13},
	number = {2},
	pages = {026027},
	author = {Francesco Negro and Silvia Muceli and Anna Margherita Castronovo and Ales Holobar and Dario Farina},
	title = {Multi-channel intramuscular and surface {EMG} decomposition by convolutive blind source separation},
	journal = {Journal of Neural Engineering},
	abstract = {Objective. The study of motor unit behavior has been classically performed by selective recording systems of muscle electrical activity (EMG signals) and decomposition algorithms able to discriminate between individual motor unit action potentials from multi-unit signals. In this study, we provide a general framework for the decomposition of multi-channel intramuscular and surface EMG signals and we extensively validate this approach with experimental recordings. Approach. First, we describe the conditions under which the assumptions of the convolutive blind separation model are satisfied. Second, we propose an approach of convolutive sphering of the observations followed by an iterative extraction of the sources. This approach is then validated using intramuscular signals recorded by novel multi-channel thin-film electrodes on the Abductor Digiti Minimi of the hand and Tibilias Anterior muscles, as well as on high-density surface EMG signals recorded by electrode grids on the First Dorsal Interosseous muscle. The validation was based on the comparison with the gold standard of manual decomposition (for intramuscular recordings) and on the two-source method (for comparison of intramuscular and surface EMG recordings) for the three human muscles and contraction forces of up to 90% MVC. Main results. The average number of common sources identified for the validation was 14 ± 7 (averaged across all trials and subjects and all comparisons), with a rate of agreement in their discharge timings of 92.8 ± 3.2%. The average Decomposability Index, calculated on the automatic decomposed signals, was 16.0 ± 2.2 (7.3–44.1). For comparison, the same index calculated on the manual decomposed signals was 15.0 ± 3.0 (6.3–76.6). Significance. These results show that the method provides a solid framework for the decomposition of multi-channel invasive and non-invasive EMG signals that allows the study of the behavior of a large number of concurrently active motor units.}
}

% CHECKED
% Studie, ze ktere moje prace vychazi
@article{luo2018convtasnet,
  author    = {Yi Luo and
               Nima Mesgarani},
  title     = {TasNet: Surpassing Ideal Time-Frequency Masking for Speech Separation},
  journal   = {CoRR},
  volume    = {abs/1809.07454},
  year      = {2018},
  month		= {Sep},
  url       = {http://arxiv.org/abs/1809.07454},
  archivePrefix = {arXiv},
  eprint    = {1809.07454},
  timestamp = {Fri, 05 Oct 2018 11:34:52 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1809-07454.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org},
  note 		= {v2}
}

	
% MSE, MAE, 
% https://towardsdatascience.com/understanding-the-3-most-common-loss-functions-for-machine-learning-regression-23e0ef3e14d3


% CHECKED
% Obrazek FeedForward NN vs Recurrent NN
@article{FFandRecNN,
  title={A comprehensive review for artificial neural network application to public transportation.},
  author={Pekel, Engin and Soner Kara, Selin},
  journal={Sigma: Journal of Engineering \& Natural Sciences/M{\"u}hendislik ve Fen Bilimleri Dergisi},
  volume={35},
  number={1},
  year={2017},
  month = {Mar},
  pages = {157-179},
}
%journal = {Sigma Journal of Engineering and Natural Sciences}


% Relu article citation about success of ReLU
@incollection{NIPS2012_4824,
title = {ImageNet Classification with Deep Convolutional Neural Networks},
author = {Alex Krizhevsky and Sutskever, Ilya and Hinton, Geoffrey E},
booktitle = {Advances in Neural Information Processing Systems 25},
editor = {F. Pereira and C. J. C. Burges and L. Bottou and K. Q. Weinberger},
pages = {1097--1105},
year = {2012},
publisher = {Curran Associates, Inc.},
url = {http://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf}
}

% CHECKED
% aktivacni funkce -- sigmoidy, softmax, relu a pod
% https://arxiv.org/abs/1811.03378
@article{nwankpa2018activation,
  author    = {Chigozie Nwankpa and
               Winifred Ijomah and
               Anthony Gachagan and
               Stephen Marshall},
  title     = {Activation Functions: Comparison of trends in Practice and Research for Deep Learning},
  journal   = {CoRR},
  volume    = {abs/1811.03378},
  year      = {2018},
  url       = {http://arxiv.org/abs/1811.03378},
  archivePrefix = {arXiv},
  eprint    = {1811.03378},
  timestamp = {Fri, 23 Nov 2018 12:43:51 +0100},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1811-03378.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

% CHECKED
%logistic sigmoid nevyhody a vyhody
%https://www.sciencedirect.com/science/article/abs/pii/0004370292900656?via%3Dihub
@article{NEAL199271,
title = "Connectionist learning of belief networks",
journal = "Artificial Intelligence",
volume = "56",
number = "1",
pages = "71 - 113",
year = "1992",
issn = "0004-3702",
doi = "https://doi.org/10.1016/0004-3702(92)90065-6",
url = "http://www.sciencedirect.com/science/article/pii/0004370292900656",
author = "Radford M. Neal",
abstract = {Connectionist learning procedures are presented for “sigmoid” and “noisy-OR” varieties of probabilistic belief networks. These networks have previously been seen primarily as a means of representing knowledge derived from experts. Here it is shown that the “Gibbs sampling” simulation procedure for such networks can support maximum-likelihood learning from empirical data through local gradient ascent. This learning procedure resembles that used for “Boltzmann machines”, and like it, allows the use of “hidden” variables to model correlations between visible variables. Due to the directed nature of the connections in a belief network, however, the “negative phase” of Boltzmann machine learning is unnecessary. Experimental results show that, as a result, learning in a sigmoid belief network can be faster than in a Boltzmann machine. These networks have other advantages over Boltzmann machines in pattern classification and decision making applications, are naturally applicable to unsupervised learning problems, and provide a link between work on connectionist learning and work on the representation of expert knowledge.}
}

% CHECKED
% https://research.google/pubs/pub40811/
% https://ieeexplore.ieee.org/abstract/document/6638312
% https://static.googleusercontent.com/media/research.google.com/en//pubs/archive/40811.pdf
% ReLU for speech processing
@inproceedings{40811,
	title	= {On Rectified Linear Units For Speech Processing},
    author={M. D. {Zeiler} and M. {Ranzato} and R. {Monga} and M. {Mao} and K. {Yang} and Q. V. {Le} and P. {Nguyen} and A. {Senior} and V. {Vanhoucke} and J. {Dean} and G. E. {Hinton}},
	year	= {2013},
    booktitle={2013 IEEE International Conference on Acoustics, Speech and Signal Processing}, 
	address	= {Vancouver, BC, Canada},
	pages={3517--3521},
	doi={10.1109/ICASSP.2013.6638312},
    ISSN={2379-190X},
	organization={IEEE},
	  abstract={Deep neural networks have recently become the gold standard for acoustic modeling in speech recognition systems. The key computational unit of a deep network is a linear projection followed by a point-wise non-linearity, which is typically a logistic function. In this work, we show that we can improve generalization and make training of deep networks faster and simpler by substituting the logistic units with rectified linear units. These units are linear when their input is positive and zero otherwise. In a supervised setting, we can successfully train very deep nets from random initialization on a large vocabulary speech recognition task achieving lower word error rates than using a logistic network with the same topology. Similarly in an unsupervised setting, we show how we can learn sparse features that can be useful for discriminative tasks. All our experiments are executed in a distributed environment using several hundred machines and several hundred hours of speech data.},
	keywords={neural nets;speech processing;speech recognition;vocabulary;speech processing;rectified linear units;deep neural networks;gold standard;acoustic modeling;key computational unit;linear projection;point-wise nonlinearity;logistic function;logistic units;supervised setting;large vocabulary speech recognition task;word error rates;sparse features;discriminative tasks;distributed environment;Logistics;Training;Neural networks;Accuracy;Unsupervised learning;Error analysis;Encoding;Rectified Linear units;Deep Learning;Neural Networks;Unsupervised Learning;Hybrid System}
}


% Obrazek Training vs Test set error
@article{trainingvstesterror,
	author = {Chen, Shang and Cheng, Ming-Jen},
	year = {2006},
	month = {Jan},
	pages = {49-58},
	title = {Building an Adaptive Evaluation System: A Design Education Application},
	volume = {3},
	journal = {Computer-Aided Design \& Applications},
	doi = {10.1080/16864360.2006.10738441}
}

% SI-SDR
% https://hal.inria.fr/inria-00544230
% https://hal.inria.fr/inria-00544230/PDF/vincent_TASLP06bis.pdf
@article{vincent-inria-00544230,
  TITLE = {{Performance measurement in blind audio source separation}},
  AUTHOR = {Vincent, Emmanuel and Gribonval, R{\'e}mi and F{\'e}votte, C{\'e}dric},
  URL = {https://hal.inria.fr/inria-00544230},
  JOURNAL = {{IEEE Transactions on Audio, Speech and Language Processing}},
  PUBLISHER = {{Institute of Electrical and Electronics Engineers}},
  VOLUME = {14},
  NUMBER = {4},
  PAGES = {1462--1469},
  YEAR = {2006},
  PDF = {https://hal.inria.fr/inria-00544230/file/vincent_TASLP06bis.pdf},
  HAL_ID = {inria-00544230},
  HAL_VERSION = {v1}
}

% CHECKED
% https://www.researchgate.net/publication/304360600_A_weighted_STOI_intelligibility_metric_based_on_mutual_information
% https://ieeexplore.ieee.org/document/7472702
@INPROCEEDINGS{7472702,
  author={L. {Lightburn} and M. {Brookes}},
  booktitle={2016 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)}, 
  title={A weighted STOI intelligibility metric based on mutual information}, 
  year={2016},
  volume={},
  number={},
  publisher = {IEEE},
  pages={5365-5369},
  abstract={It is known that the information required for the intelligibility of a speech signal is distributed non-uniformly in time. In this paper we propose WSTOI, a modified version of STOI, a speech intelligibility metric. With WSTOI the contribution of each time-frequency cell is weighted by an estimate of its intelligibility content. This estimate is equal to the mutual information between two hypothetical signals at either end of a simplified model of human communication. Listening tests show that the modification improves the prediction accuracy of STOI at all performance levels on both long and short utterances. An improvement was observed across all tested noise types and suppression algorithms.},
  keywords={signal denoising;speech processing;time-frequency analysis;weighted STOI intelligibility metric;mutual information;speech signal;time-frequency cell;hypothetical signals;human communication;performance levels;suppression algorithms;noise types;Speech;Measurement;Mutual information;Computational modeling;Correlation;Production;Information rates;Intelligibility;intelligibility metric;intelligibility estimate;mutual information;speech entropy},
  doi={10.1109/ICASSP.2016.7472702},
  ISSN={2379-190X},
  month={Mar}
}


% CHECKED
% Obrazek PESQ a popis pesq
% https://arxiv.org/abs/1901.09146
% https://arxiv.org/pdf/1901.09146.pdf
@article{sdr-pesq-optimization,
  author    = {Jaeyoung Kim and
               Mostafa El{-}Khamy and
               Jungwon Lee},
  title     = {End-to-End Multi-Task Denoising for joint {SDR} and {PESQ} Optimization},
  journal   = {CoRR},
  volume    = {abs/1901.09146},
  year      = {2019},
  month 	= {Jan},
  url       = {http://arxiv.org/abs/1901.09146},
  archivePrefix = {arXiv},
  eprint    = {1901.09146},
  timestamp = {Sat, 02 Feb 2019 16:56:00 +0100},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1901-09146.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

% CHECKED
% STOI a SDR metriky
% https://arxiv.org/abs/1806.00511
% https://arxiv.org/pdf/1806.00511.pdf
% https://ieeexplore.ieee.org/abstract/document/8659758
@inproceedings{venkataramani2018performance,
  title={Performance based cost functions for end-to-end speech separation},
  author={S. {Venkataramani} and R. {Higa} and P. {Smaragdis}},
  booktitle={2018 Asia-Pacific Signal and Information Processing Association Annual Summit and Conference (APSIPA ASC)},
  pages={350--355},
  year={2018},
  organization={IEEE},
  abstract={Recent neural network strategies for source separation attempt to model audio signals by processing their waveforms directly. Mean squared error (MSE) that measures the Euclidean distance between waveforms of denoised speech and the ground-truth speech, has been a natural cost-function for these approaches. However, MSE is not a perceptually motivated measure and may result in large perceptual discrepancies. In this paper, we propose and experiment with new loss functions for end-to-end source separation. These loss functions are motivated by BSS_Eval and perceptual metrics like source to distortion ratio (SDR), source to interference ratio (SIR), source to artifact ratio (SAR) and short-time objective intelligibility ratio (STOI). This enables the flexibility to mix and match these loss functions depending upon the requirements of the task. Subjective listening tests reveal that combinations of the proposed cost functions help achieve superior separation performance as compared to stand-alone MSE and SDR costs.},
  keywords={audio signal processing;hearing;interference (signal);mean square error methods;neural nets;signal denoising;source separation;speech processing;end-to-end speech separation;audio signals;mean squared error;Euclidean distance;denoised speech;ground-truth speech;end-to-end source separation;short-time objective intelligibility ratio;neural network strategies;BSS_Eval;source to distortion ratio;source to interference ratio;source to artifact ratio;Source separation;Cost function;Measurement;Spectrogram;Neural networks;Interference;Convolution;End-to-end speech separation;Deep learning;Cost functions},
  doi={10.23919/APSIPA.2018.8659758},
  ISSN={2640-0103},
  month={Nov}
}

% CHECKED
% STOI
% https://ieeexplore.ieee.org/document/5495701
% http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.696.995
@INPROCEEDINGS{taal-stoi,
  author={C. H. {Taal} and R. C. {Hendriks} and R. {Heusdens} and J. {Jensen}},
  booktitle={2010 IEEE International Conference on Acoustics, Speech and Signal Processing}, 
  title={A short-time objective intelligibility measure for time-frequency weighted noisy speech}, 
  year={2010},
  volume={},
  number={},
  pages={4214-4217},
  abstract={Existing objective speech-intelligibility measures are suitable for several types of degradation, however, it turns out that they are less appropriate for methods where noisy speech is processed by a time-frequency (TF) weighting, e.g., noise reduction and speech separation. In this paper, we present an objective intelligibility measure, which shows high correlation (rho=0.95) with the intelligibility of both noisy, and TF-weighted noisy speech. The proposed method shows significantly better performance than three other, more sophisticated, objective measures. Furthermore, it is based on an intermediate intelligibility measure for short-time (approximately 400 ms) TF-regions, and uses a simple DFT-based TF-decomposition. In addition, a free Matlab implementation is provided.},
  keywords={speech enhancement;speech intelligibility;time-frequency weighted noisy speech;objective speech-intelligibility measurement;time-frequency weighting;noise reduction;speech separation;objective intelligibility measurement;Weight measurement;Time frequency analysis;Speech processing;Speech enhancement;Degradation;Artificial intelligence;Noise reduction;Signal processing;Noise measurement;Testing;intelligibility prediction;speech enhancement;noisy speech},
  doi={10.1109/ICASSP.2010.5495701},
  ISSN={2379-190X},
  month={Mar}
}

% CHECKED
% STOI definition
% https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4452133/
% https://ieeexplore.ieee.org/abstract/document/5713237
@ARTICLE{5713237,
  author={C. H. {Taal} and R. C. {Hendriks} and R. {Heusdens} and J. {Jensen}},
  journal={IEEE Transactions on Audio, Speech, and Language Processing}, 
  title={An Algorithm for Intelligibility Prediction of Time–Frequency Weighted Noisy Speech}, 
  year={2011},
  volume={19},
  number={7},
  pages={2125-2136},
  abstract={In the development process of noise-reduction algorithms, an objective machine-driven intelligibility measure which shows high correlation with speech intelligibility is of great interest. Besides reducing time and costs compared to real listening experiments, an objective intelligibility measure could also help provide answers on how to improve the intelligibility of noisy unprocessed speech. In this paper, a short-time objective intelligibility measure (STOI) is presented, which shows high correlation with the intelligibility of noisy and time-frequency weighted noisy speech (e.g., resulting from noise reduction) of three different listening experiments. In general, STOI showed better correlation with speech intelligibility compared to five other reference objective intelligibility models. In contrast to other conventional intelligibility models which tend to rely on global statistics across entire sentences, STOI is based on shorter time segments (386 ms). Experiments indeed show that it is beneficial to take segment lengths of this order into account. In addition, a free Matlab implementation is provided.},
  keywords={speech enhancement;speech intelligibility;intelligibility prediction;time-frequency weighted noisy speech;noise-reduction algorithm;objective machine-driven intelligibility measure;speech intelligibility;noisy unprocessed speech;short-time objective intelligibility measure;objective intelligibility model;Speech;Noise measurement;Correlation;Speech processing;Signal to noise ratio;Time frequency analysis;Noise reduction;objective measure;speech enhancement;speech intelligibility prediction},
  doi={10.1109/TASL.2011.2114881},
  ISSN={1558-7924},
  month={Sep}
}

% CHECKED
% Xavier algorithm for weight initialisation
% 	http://proceedings.mlr.press/v9/glorot10a.html
% 		http://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf
@InProceedings{pmlr-v9-glorot10a,
	title = 	 {Understanding the difficulty of training deep feedforward neural networks},
	author = 	 {Xavier Glorot and Yoshua Bengio},
	booktitle = 	 {Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics},
	pages = 	 {249--256},
	year = 	 	 {2010},
	editor = 	 {Yee Whye Teh and Mike Titterington},
	volume = 	 {9},
	series = 	 {Proceedings of Machine Learning Research},
	address = 	 {Chia Laguna Resort, Sardinia, Italy},
	month = 	 {13--15 May},
	publisher = 	 {PMLR},
	pdf = 	 {http://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf},
	url = 	 {http://proceedings.mlr.press/v9/glorot10a.html},
	abstract = 	 {Whereas before 2006 it appears that deep multi-layer neural networks were not successfully trained, since then several algorithms have been shown to successfully train them, with experimental results showing the superiority of deeper vs less deep architectures. All these experimental results were obtained with new initialization or training mechanisms. Our objective here is to understand better why standard gradient descent from random initialization is doing so poorly with deep neural networks, to better understand these recent relative successes and help design better algorithms in the future.  We first observe the influence of the non-linear activations functions. We find that the logistic sigmoid activation is unsuited for deep networks with random initialization because of its mean value, which can drive especially the top hidden layer into saturation. Surprisingly, we find that saturated units can move out of saturation by themselves, albeit slowly, and explaining the plateaus sometimes seen when training neural networks. We find that a new non-linearity that saturates less can often be beneficial. Finally, we study how activations and gradients vary across layers and during training, with the idea that training may be more difficult when the singular values of the Jacobian associated with each layer are far from 1.  Based on these considerations, we propose a new initialization scheme that brings substantially faster convergence.}
}


% CHECKED
% [1] ze studie
@article{speechseparationoverview,
	author = {Wang, DeLiang and Chen, Jitong},
	title = {Supervised Speech Separation Based on Deep Learning: An Overview},
	year = {2018},
	issue_date = {October 2018},
	publisher = {IEEE Press},
	volume = {26},
	number = {10},
	issn = {2329-9290},
	url = {https://doi.org/10.1109/TASLP.2018.2842159},
	doi = {10.1109/TASLP.2018.2842159},
	journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
	month = oct,
	pages = {1702–1726},
	numpages = {25}
}

% CHECKED
% [23] ze studie -- TCN
% https://arxiv.org/abs/1608.08242
% https://link.springer.com/chapter/10.1007/978-3-319-49409-8_7
@InProceedings{lea2016temporal,
author={Lea, Colin and Vidal, Ren{\'e} and Reiter, Austin and Hager, Gregory D.},
editor={Hua, Gang and J{\'e}gou, Herv{\'e}},
title="Temporal Convolutional Networks: A Unified Approach to Action Segmentation",
booktitle="Computer Vision -- ECCV 2016 Workshops",
year="2016",
publisher="Springer International Publishing",
address="Cham",
pages="47--54",
abstract={The dominant paradigm for video-based action segmentation is composed of two steps: first, compute low-level features for each frame using Dense Trajectories or a Convolutional Neural Network to encode local spatiotemporal information, and second, input these features into a classifier such as a Recurrent Neural Network (RNN) that captures high-level temporal relationships. While often effective, this decoupling requires specifying two separate models, each with their own complexities, and prevents capturing more nuanced long-range spatiotemporal relationships. We propose a unified approach, as demonstrated by our Temporal Convolutional Network (TCN), that hierarchically captures relationships at low-, intermediate-, and high-level time-scales. Our model achieves superior or competitive performance using video or sensor data on three public action segmentation datasets and can be trained in a fraction of the time it takes to train an RNN.},
isbn="978-3-319-49409-8",
url={https://arxiv.org/abs/1608.08242}
}

% CHECKED
% [25] ze studie -- TCN
% https://arxiv.org/abs/1803.01271
@article{bai2018empirical,
  author    = {Shaojie Bai and
               J. Zico Kolter and
               Vladlen Koltun},
  title     = {An Empirical Evaluation of Generic Convolutional and Recurrent Networks
               for Sequence Modeling},
  journal	= {arXiv preprint arXiv:1803.01271},
  volume    = {abs/1803.01271},
  year      = {2018},
  url       = {http://arxiv.org/abs/1803.01271},
  archivePrefix = {arXiv},
  eprint    = {1803.01271},
  timestamp = {Mon, 13 Aug 2018 16:47:39 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1803-01271.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

% CHECKED
%Batch normalization citace ze studie [47]
% https://pytorch.org/docs/master/generated/torch.nn.BatchNorm1d.html
%	https://arxiv.org/abs/1502.03167
% 		https://arxiv.org/pdf/1502.03167.pdf
@article{ioffe2015batch,
  author    = {Sergey Ioffe and
               Christian Szegedy},
  title     = {Batch Normalization: Accelerating Deep Network Training by Reducing
               Internal Covariate Shift},
  journal	= {arXiv preprint arXiv:1502.03167},
  volume    = {abs/1502.03167},
  year      = {2015},
  url       = {http://arxiv.org/abs/1502.03167},
  archivePrefix = {arXiv},
  eprint    = {1502.03167},
  timestamp = {Mon, 13 Aug 2018 16:47:06 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/IoffeS15.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

% CHECKED
%Layer normalization z pytorch, ale taky pouzita na Batch Norm
% https://pytorch.org/docs/master/generated/torch.nn.LayerNorm.html
% 	https://arxiv.org/abs/1607.06450
@article{ba2016layer,
  title={Layer normalization},
  author={Ba, Jimmy Lei and Kiros, Jamie Ryan and Hinton, Geoffrey E},
  journal={arXiv preprint arXiv:1607.06450},
  keywords = {Statistics - Machine Learning, Computer Science - Machine Learning},
  year = {2016},
  month = {Jul},
  url={https://arxiv.org/abs/1607.06450}
}

% Web o normalizaci
%	https://mlexplained.com/2018/11/30/an-overview-of-normalization-methods-in-deep-learning/
% ------


% CHECKED
%PReLU paper
% https://arxiv.org/abs/1502.01852
@INPROCEEDINGS{he2015delving,
  author={K. {He} and X. {Zhang} and S. {Ren} and J. {Sun}},
  booktitle={2015 IEEE International Conference on Computer Vision (ICCV)}, 
  title={Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification}, 
  year={2015},
  volume={},
  number={},
  pages={1026-1034},
  abstract={Rectified activation units (rectifiers) are essential for state-of-the-art neural networks. In this work, we study rectifier neural networks for image classification from two aspects. First, we propose a Parametric Rectified Linear Unit (PReLU) that generalizes the traditional rectified unit. PReLU improves model fitting with nearly zero extra computational cost and little overfitting risk. Second, we derive a robust initialization method that particularly considers the rectifier nonlinearities. This method enables us to train extremely deep rectified models directly from scratch and to investigate deeper or wider network architectures. Based on the learnable activation and advanced initialization, we achieve 4.94% top-5 test error on the ImageNet 2012 classification dataset. This is a 26% relative improvement over the ILSVRC 2014 winner (GoogLeNet, 6.66% [33]). To our knowledge, our result is the first to surpass the reported human-level performance (5.1%, [26]) on this dataset.},
  keywords={image classification;neural nets;human-level performance;ILSVRC 2014 winner;ImageNet 2012 classification dataset;network architectures;rectifier nonlinearities;robust initialization method;overfitting risk;model fitting;PReLU;parametric rectified linear unit;rectifier neural networks;state-of-the-art neural networks;rectified activation units;ImageNet classification;Training;Computational modeling;Adaptation models;Testing;Gaussian distribution;Biological neural networks},
  doi={10.1109/ICCV.2015.123},
  ISSN={2380-7504},
  month={Dec},
  url={https://arxiv.org/abs/1502.01852}
}


% CHECKED
%Residual connection from TasNet
% https://arxiv.org/abs/1512.03385
% https://ieeexplore.ieee.org/document/7780459
@INPROCEEDINGS{he2015deep,
  author={K. {He} and X. {Zhang} and S. {Ren} and J. {Sun}},
  booktitle={2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)}, 
  title={Deep Residual Learning for Image Recognition}, 
  year={2016},
  volume={},
  number={},
  pages={770-778},
  abstract={Deeper neural networks are more difficult to train. We present a residual learning framework to ease the training of networks that are substantially deeper than those used previously. We explicitly reformulate the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions. We provide comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth. On the ImageNet dataset we evaluate residual nets with a depth of up to 152 layers - 8× deeper than VGG nets [40] but still having lower complexity. An ensemble of these residual nets achieves 3.57% error on the ImageNet test set. This result won the 1st place on the ILSVRC 2015 classification task. We also present analysis on CIFAR-10 with 100 and 1000 layers. The depth of representations is of central importance for many visual recognition tasks. Solely due to our extremely deep representations, we obtain a 28% relative improvement on the COCO object detection dataset. Deep residual nets are foundations of our submissions to ILSVRC & COCO 2015 competitions1, where we also won the 1st places on the tasks of ImageNet detection, ImageNet localization, COCO detection, and COCO segmentation.},
  keywords={image classification;learning (artificial intelligence);neural nets;object detection;COCO segmentation;ImageNet localization;ILSVRC & COCO 2015 competitions;deep residual nets;COCO object detection dataset;visual recognition tasks;CIFAR-10;ILSVRC 2015 classification task;ImageNet test set;VGG nets;residual nets;ImageNet dataset;residual function learning;deeper neural network training;image recognition;deep residual learning;Training;Degradation;Complexity theory;Image recognition;Neural networks;Visualization;Image segmentation},
  doi={10.1109/CVPR.2016.90},
  ISSN={1063-6919},
  month={Jun},
  url={https://arxiv.org/abs/1512.03385}
}

% CHECKED
%Why Residual Networks Can Become Extremely Deep
% https://arxiv.org/abs/1805.07477
@ARTICLE{zaeemzadeh2018normpreservation,
  author={A. {Zaeemzadeh} and N. {Rahnavard} and M. {Shah}},
  journal={IEEE Transactions on Pattern Analysis and Machine Intelligence}, 
  title={Norm-Preservation: Why Residual Networks Can Become Extremely Deep?}, 
  year={2020},
  volume={},
  number={},
  pages={1-1},
  abstract={Augmenting neural networks with skip connections, as introduced in the so-called ResNet architecture, surprised the community by enabling the training of networks of more than 1,000 layers with significant performance gains. This paper deciphers ResNet by analyzing the effect of skip connections, and puts forward new theoretical results on the advantages of identity skip connections in neural networks. We prove that the skip connections in the residual blocks facilitate preserving the norm of the gradient, and lead to stable back-propagation, which is desirable from optimization perspective. We also show that, perhaps surprisingly, as more residual blocks are stacked, the norm-preservation of the network is enhanced. Our theoretical arguments are supported by extensive empirical evidence. Can we push for extra norm-preservation? We answer this question by proposing an efficient method to regularize the singular values of the convolution operator and making the ResNet's transition layers extra norm-preserving. Our numerical investigations demonstrate that the learning dynamics and the classification performance of ResNet can be improved by making it even more norm preserving. Our results and the introduced modification for ResNet, referred to as Procrustes ResNets, can be used as a guide for training deeper networks and can also inspire new deeper architectures.},
  keywords={Optimization;Training;Residual neural networks;Convolution;Numerical stability;Computer architecture;Residual Networks;Convolutional Neural Networks;Optimization Stability;Norm Preservation;Spectral Regularization},
  doi={10.1109/TPAMI.2020.2990339},
  ISSN={1939-3539},
  month={},
  url={https://arxiv.org/abs/1805.07477v5}
}

% CHECKED
%Backpropagation Applied to Handwritten Zip Code Recognition (CNN)
% https://ieeexplore.ieee.org/document/6795724
@ARTICLE{6795724,
  author={Y. {LeCun} and B. {Boser} and J. S. {Denker} and D. {Henderson} and R. E. {Howard} and W. {Hubbard} and L. D. {Jackel}},
  journal={Neural Computation}, 
  title={Backpropagation Applied to Handwritten Zip Code Recognition}, 
  year={1989},
  volume={1},
  number={4},
  pages={541-551},
  abstract={The ability of learning networks to generalize can be greatly enhanced by providing constraints from the task domain. This paper demonstrates how such constraints can be integrated into a backpropagation network through the architecture of the network. This approach has been successfully applied to the recognition of handwritten zip code digits provided by the U.S. Postal Service. A single network learns the entire recognition operation, going from the normalized image of the character to the final classification.},
  keywords={},
  doi={10.1162/neco.1989.1.4.541},
  ISSN={0899-7667},
  month={Dec},
  publisher = {MITP}
}

% CHECKED
%Learning long-term dependencies with gradient descent is difficult
% https://ieeexplore.ieee.org/document/279181
@ARTICLE{279181,
  author={Y. {Bengio} and P. {Simard} and P. {Frasconi}},
  journal={IEEE Transactions on Neural Networks}, 
  title={Learning long-term dependencies with gradient descent is difficult}, 
  year={1994},
  volume={5},
  number={2},
  pages={157-166},
  abstract={Recurrent neural networks can be used to map input sequences to output sequences, such as for recognition, production or prediction problems. However, practical difficulties have been reported in training recurrent neural networks to perform tasks in which the temporal contingencies present in the input/output sequences span long intervals. We show why gradient based learning algorithms face an increasingly difficult problem as the duration of the dependencies to be captured increases. These results expose a trade-off between efficient learning by gradient descent and latching on information for long periods. Based on an understanding of this problem, alternatives to standard gradient descent are considered.<>},
  keywords={recurrent neural nets;learning (artificial intelligence);numerical analysis;long-term dependencies;gradient descent;recognition;production problems;prediction problems;recurrent neural network training;temporal contingencies;input/output sequence mapping;efficient learning;Recurrent neural networks;Production;Delay effects;Intelligent networks;Neural networks;Discrete transforms;Computer networks;Cost function;Neurofeedback;Displays},
  doi={10.1109/72.279181},
  ISSN={1941-0093},
  month={Mar}
}


% CHECKED
% Tlusta NN knizka z knihovny
@BOOK{mitdeeplearning,
	author =       "Ian Goodfellow and Yoshua Bengio and Aaron Courville",
	title =		   {Deep Learning},
	publisher =    "The MIT Press",
	year =         "2016",
	isbn =         "9780262035613",
	note={\url{http://www.deeplearningbook.org}},
}
   
% CHECKED 
% Ta tenka knizka, ale ctivejsi o NN, CNN, atd z knihovny
@Book{mitdeeplearning_small,
 author = {Kelleher, John},
 title = {Deep learning / John D. Kelleher},
 publisher = {The MIT Press},
 year = {2019},
 month = {Aug},
 address = {Cambridge, Massachusetts, London, England},
 isbn = {9780262537551},
 series = {The MIT Press Essential Knowledge series}
 }
	
% CHECKED
% PESQ - itut
% Rix, A. W., Beerends, J. G., Hollier, M. P., and Hekstra,A. P.  Perceptual evaluation of speech quality (pesq)-anew method for speech quality assessment of telephonenetworks and codecs.  InAcoustics, Speech, and SignalProcessing, 2001. Proceedings.(ICASSP’01). 2001 IEEEInternational Conference on,  volume  2,  pp.  749–752.IEEE, 2001"
%
% https://ieeexplore.ieee.org/document/941023
% https://www.researchgate.net/publication/3908525_Perceptual_Evaluation_of_Speech_Quality_PESQ_A_New_Method_for_Speech_Quality_Assessment_of_Telephone_Networks_and_Codecs
@inproceedings{pesq-itut,
author = {Rix, Antony and Beerends, John and Hollier, Michael and Hekstra, Andries},
year = {2001},
month = {Feb},
pages = {749-752 vol.2},
title = {Perceptual Evaluation of Speech Quality (PESQ): A New Method for Speech Quality Assessment of Telephone Networks and Codecs},
volume = {2},
isbn = {0-7803-7041-4},
journal = {ICASSP, IEEE International Conference on Acoustics, Speech and Signal Processing - Proceedings},
doi = {10.1109/ICASSP.2001.941023}
}
%@INPROCEEDINGS{pesq-itut,
%  author={A. W. {Rix} and J. G. {Beerends} and M. P. {Hollier} and A. P. {Hekstra}},
%  booktitle={2001 IEEE International Conference on Acoustics, Speech, and Signal Processing. Proceedings (Cat. No.01CH37221)}, 
%  title={Perceptual evaluation of speech quality (PESQ)-a new method for speech quality assessment of telephone networks and codecs}, 
%  year={2001},
%  volume={2},
%  number={},
%  pages={749-752 vol.2},
%  abstract={Previous objective speech quality assessment models, such as bark spectral distortion (BSD), the perceptual speech quality measure (PSQM), and measuring normalizing blocks (MNB), have been found to be suitable for assessing only a limited range of distortions. A new model has therefore been developed for use across a wider range of network conditions, including analogue connections, codecs, packet loss and variable delay. Known as perceptual evaluation of speech quality (PESQ), it is the result of integration of the perceptual analysis measurement system (PAMS) and PSQM99, an enhanced version of PSQM. PESQ is expected to become a new ITU-T recommendation P.862, replacing P.861 which specified PSQM and MNB.},
%  keywords={speech intelligibility;speech codecs;telecommunication equipment testing;packet switching;delays;telephone networks;perceptual evaluation of speech quality;telephone networks;perceptual speech quality measure;measuring normalizing blocks;analogue connections;packet loss;variable delay;perceptual analysis measurement system;PAMS;PSQM99;ITU-T recommendation P.862;speech codecs;objective speech quality assessment models;Speech analysis;Quality assessment;Distortion measurement;Nonlinear distortion;Nonlinear filters;Telephony;Signal processing;Delay effects;Speech codecs;Degradation},
%  doi={10.1109/ICASSP.2001.941023},
%  ISSN={1520-6149},
%  month={May},
%}


% CHECKED
% ze studie odkaz [48] na trenovaci dataset Journal
% Deep clustering: Discriminative embeddings for segmentation and separation
% https://arxiv.org/abs/1508.04306
% https://arxiv.org/pdf/1508.04306.pdf
@inproceedings{DBLP-dataset,
  title={Deep clustering: Discriminative embeddings for segmentation and separation},
  author={Hershey, John R and Chen, Zhuo and Le Roux, Jonathan and Watanabe, Shinji},
  booktitle={2016 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
  pages={31--35},
  year={2016},
  organization={IEEE},
  url = {http://arxiv.org/abs/1508.04306},
}

% CHECKED
% An overview of gradient descent optimization algorithms
% https://arxiv.org/abs/1609.04747
@article{ruder2016overview,
    title={An overview of gradient descent optimization algorithms},
    author={Sebastian Ruder},
    year={2016},
    eprint={1609.04747},
    archivePrefix={arXiv},
    primaryClass={cs.LG},
    journal={ArXiv},
	volume={abs/1609.04747},
	url = {http://arxiv.org/abs/1609.04747},
}

